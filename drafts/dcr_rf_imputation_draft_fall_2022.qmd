---
title: |
  Giving the leaves back to the forest
subtitle: A primer on the use of random forest models as chained equations for imputing missing data
short-title: Giving the leaves back to the forest
repo: "Replication materials are stored at <https://github.com/DamonCharlesRoberts/wid-and-gender>"
author:
  - name: Damon C. Roberts
    email: damon.roberts-1@colorado.edu
    orcid: 0000-0002-4360-3675
    title: PhD Candidate
    affiliations:
      - id: CU
        name: University of Colorado Boulder
        department: Political Science
        address: 333 UCB
        city: Boulder
        region: CO 
        postal-code: 80309-0333
    attributes:
      corresponding: true   
abstract: |
    Political scientists often struggle with decisions about what to do with incomplete cases for their regression analyses, and one can often make several decisions that influence oneâ€™s ultimate substantive conclusions. In many areas of research outside of political science and the social sciences, scholars take advantage of an extension of multiple imputation, which offers the choice to leverage machine learning models for predicting values in missing data. This manuscript provides a summary of missing data and its consequences for our regression models along with providing an explanation of how to implement random forest models with an expanded form of the multiple imputation procedure, called multiple imputation with chained equation to handle complex causes for non-random missingness in our data. After providing a primer on standard missing data procedures in political science and random forest with multiple imputation with chained equations, I examine its performance on simulated data and data from the 2020 American National Election Study. I conclude by providing recommendations for dealing with missing data in practice. 
thanks: |
    I would like to thank Andrew Q. Philips and Jennifer Wolak for their advice and many conversations during the development of this project as well as Andy Baker for offering me the space to write the manuscript and for his feedback. I would also like to thank the discussants and panelists at MPSA for their useful feedback and encouragement.
nonblind: blind
keywords:
  - Missing data
  - Multiple imputation
  - Machine learning
date: today
bibliography: ../assets/references.bib
format:
  pdf:
    template: "../../templates/manuscripts/apsr_template.tex"
    cite-method: biblatex
execute:
    echo: false
    message: false
    warning: false
---
{{< pagebreak >}}
<!-- 
    Title: RF Imputation in psci
    Notes:
        - Description: QMD Script to run code and to generate manuscript
        - Updated: 2023-01-18
        - Updated by: dcr
        - Code:
            * Make it more efficient!
            * Take an angle of testing performance and efficiency of different procedures!
        - Writing:
            * After running it, interpret the results.
            * GET A DRAFT DONE
--->

# Introduction

Missing values are common in social science data. @king_et-al_2001 estimate that political scientists lose about one-third of their data in their complete case regression analyses due to missing data. There are many reasons why missing values arise in our data. In surveys, these are referred to as item-nonresponse and pose threats to obtaining unbiased estimates for public opinion research when researchers utilize listwise deletion (LWD) in their regression analyses [@weisberg_2005]; mainly when the researcher can predict the cause of the missingness with an observed cause - which scholars refer to the data as ``missing at random" [@king_et-al_2001].^[Which is a common occurrence, more so than missingness caused entirely by random chance - which, if one meets this condition, gives unbiased estimates with LWD.] More generally, missing values come about in other types of data due to the unit's (e.g., country, respondent, politician) attempt to obfuscate information, data collector error, or researcher error. When common and not from a stochastic data generating process (DGP), missing data pose threats to causal inference either through model identification with non-full rank matrices or through sample representativeness and the challenges that create claims about causal inference.

Unsurprisingly, several political scientists generate tools and learn from other fields to deal with these challenges due to the severity of the consequences for inference. There are four primary approaches that political scientists use in dealing with missing data: (1) listwise deletion or complete case analysis (LWD), (2) simple mean or median based imputation, (3) hot deck or regression-based imputation, or (4) multiple imputation (MI). Many of the most popular approaches to imputation carry assumptions that scholars have to satisfy. This manuscript aims to provide a primer to political scientists on the use of machine learning models in the Multiple Imputation with Chained Equations (`MICE`) framework - an extension of the familiar MI approach. 

Machine learning with `MICE` is not a new approach. While a JSTOR search of Political Science journals reports a handful of articles that discuss this particular approach, it is certainly not widely used by political scientists. 

As the present article discusses, `MICE` allows the researcher to choose various models to aid the imputation process. Selecting a model with its underlying assumptions provides many benefits for researchers in choosing a procedure that is most appropriate for their particular circumstances. Choosing models that allow for flexibility and models that do not have a number of restrictive assumptions benefit those unsure about the exact data-generating process of the missingness. The specific emphasis in this paper on using random forest models in `MICE` (`RF-MICE`) results from the strong preference of those who use machine learning models for predictive regressions to use random forest models due to their flexibility as a result of their fully non-parametric nature and for their performance under many different, and complex, circumstances. Fields like the biomedical sciences treat missing values as out-of-sample predictions that random forest models predict; they see the purpose of imputation as aligned with a task that random forest models are optimal. As other fields use this `RF-MICE` procedure, there are implementations in STATA, R, and Python. All of which are relatively easy in terms of knowledge to code in either of these languages [@buuren_goothuis-oudshoorn_2011]. To be clear, this manuscript argues that we should not only consider using the `MICE` framework for imputation but also should consider using Random Forest models.

This manuscript compares the utility of random forest models in the `MICE` procedure for imputation to other common imputation tools used in political science. In doing this, the manuscript encourages political scientists to consider this procedure when faced with conditions where missing data are present, and the other common tools seem unsuitable. It is not to paint these models as superior in absolute terms to other procedures for imputation. To do that is a waste of time given the variety of circumstances where some methodological tools are suitable in some circumstances and not for others. This manuscript provides a primer encouraging political scientists to consider this tool when faced with missing data and to give them enough background so they may be comfortable using it. Furthermore, the manuscript agrees with the common recommendation that practitioners should recognize the value of reducing one's dependence on a single procedure and contends that one must consider using multiple procedures to reduce the dependency of one's results on any given procedure. The replication code for the applied analysis accompanying this manuscript also demonstrates how to implement this procedure.

The next section reviews common approaches handling missing data that political scientists currently use. The next section describes machine learning and random forest models and links this to my claim of their utility for predicting missing data when used in the `MICE` procedure. I then move into applied examples where I examine the performance of these random forest models to other common approaches to dealing with missing data on simulated data and an applied example with the $2020$ American National Election Study. I then discuss recommendations for when one should use the reigning popular techniques or the random forest application in the `MICE` procedure. 

<!-- 
    END OF INTRODUCTION SECTION
-->

# Types of missing data, imputation, and MI in Political Science

## MCAR, MAR, and MNAR

Missing data arise in different forms. Researchers describe missing data in three ways - often using somewhat unintuitive acronyms. The first form missing data takes is Missing Completely At Random (MCAR). This means that the data generating process for the missingness is random - there are no observed or unobserved causes of missingness. The second form missing data takes is Missing At Random (MAR). In MAR, these data are missing due to some observed cause. Some argue that MAR is much more common given the state of how large most contemporary social science data sets are [@schunk_2008]. The third form that missing data take is Missing Not At Random (MNAR)^[Sometimes called Non-ignorable (NI).]. MNAR happens when observed and unobserved causes explain the missingness. What distinguishes MNAR from MAR is that the researcher does not have a clear path forward to handle the cause of missingness. This occurs either because the variable where there is missingness is, itself, a cause of the missingness, or data explaining the cause of missingness is unobserved by the researcher. Since missing data take different forms, researchers use a few different approaches to deal with these challenges.

## Dealing with missingness
    
At the time of writing, @king_et-al_2001 estimated that 94% of political scientists use LWD to deal with missing data. In short, LWD does not seek to impute missing values. Instead, if the DV or any of the covariates in a regression model for a given observation are missing, the researcher does not include that observation in the analysis. Traditionally, scholars argue that LWD performs best (in terms of reducing the resulting bias in the researcher's subsequent regression models) when the data are MCAR. 

If the data are MAR or MNAR, deleting observations with missing data introduces bias in one's regression estimates through a failure to account for correlation between the independent variable(s) and the error [@king_et-al_2001, @weisberg_2005, @schunk_2008, @azur_et-al_2011]. Furthermore, it has the potential to decrease statistical power. A meta-analysis of comparative and international political economy papers that use LWD demonstrates that political scientists have much, upwards of 50%, more Type I error - an incorrect rejection of the null hypothesis - than we would expect as a result of how we implement LWD [@lall_2016]. 

Others push against this claim and instead argue that LWD does not inherently generate bias for non-MCAR data but that researchers neglect to control for the cause of MAR or MNAR [@arel-bundock_pelc_2018]. This is still dependent on the researcher's grasp of theory and ability to identify the DGP leading to the missingness. Though the onus is on the researcher to do this, it is often a relatively high standard given the complexity to which social phenomena relate and the tendency for our datasets to be highly-dimensional. Furthermore, this also increases the number of parameters one must include in their statistical models - which runs the risk of increasing collinearity [@schrodt_2014_jorp].

Like LWD, simple imputation techniques like mean-and-median-based imputation do not reduce the chances of biased regression estimates. These approaches, called interpolation and extrapolation, are common for panel data and cross-sectional time series data. If you have missing data for an observation in one panel, you can take the same observations' responses in a previous panel and a latter panel. You then take the mean or the median of that particular observation for that variable. Other approaches seek to reduce this MAR-based bias through conditioning on other variables.

Hot Deck approaches to imputing missingness are regression-based in that they define the dependent variable as the one the researcher is attempting to impute and use variables thought to predict the cause of missingness in MAR contexts [@schunk_2008]. In this approach, you often use a few variables to condition on. In many cases, the precise mechanism generating missingness is often tricky to triangulate. As a result, if you fail to provide the correct model specification when the data are MAR, you often end up with biased regression estimates. 

MI seeks to solve this issue by using the entire dataset for imputing missing values [@rubin_1996]. This approach uses the other variables in the dataset to generate a joint posterior distribution of all possible missing values for that particular observation. Many assume that most social science data sets are sufficiently large enough to condition on the mechanism generating MAR [@schunk_2008]. Unlike the other approaches, MI also generates uncertainty around the imputed values - via its construction of the joint posterior distribution [@rubin_1996] - which enables the researcher to be more transparent about the validity of those imputed values and to include that uncertainty in the researcher's subsequent statistical analyses [@king_et-al_2001, @von-hippel_2015]. A prevalent implementation of MI in political science is Honaker, King, and Blackwell's @honaker_et-al_2011] `AMELIA II` software [@lall_2016]. This useful tool provides a computationally fast and simple process for imputation. Compared to the other approaches to missing data, `AMELIA II` performs quite well [@honaker_et-al_2011, @kropko_et-al_2014]. MI, however, often requires a set of distributional assumptions for the joint distribution - often the multivariate normal [@honaker_et-al_2011].

There is a variant to MI that seeks more computational efficiency and loosens some of the distributional assumptions required. This variant is called Multiple Imputation through Chained Equations (`MICE`). `MICE` performs quite well for large imputation tasks. MI struggles to impute values when there is missingness in the other variables of the dataset [@kropko_et-al_2014] - which is quite common. Though not reliant on a multivariate normal distribution, Conditional MI still relies on generalized linear models (GLM) in calculating the values. `MICE` tries to get around this problem in a few steps, as described by @azur_et-al_2011. @fig-process provides a visual representation of the procedure for a form of `MICE` used in this manuscript. I include more details about random forest models in the following subsection.

```{mermaid}
%%| label: Steps of RF-MICE procedure
%%| fig-cap: Snap-judgement model
%%| fig-width: 8
%%{init: {'theme':'base', 'themeVariables':{'primaryColor':'#ffffff', 'primaryBorderColor':'#000000'}}}%%
graph TD;
A[Missing Data] --> B[Simple Imputation]
B --> C[Select Variable and remove imputed values]
C --> D[Random forest model to impute predicted values]
D --> |Repeat for each variable| C
```

First, `MICE` performs a simple imputation, or interpolation, for every missing value in the entire dataset. These are the placeholder values. The second step in the general `MICE` paradigm involves identifying one variable to impute. Once complete, it then removes those placeholder values. The third step then involves regressing the observed values of the variable on the other variables in the model and replacing the predicted values generated from the regression model for the missing values. The fourth step is to repeat steps two and three for each variable in the data set with missing values - this constitutes a single iteration. As a fifth step, you perform between five and ten iterations^[Though, the exact number of recommended iterations used in `MICE` are still up for debate [see @buuren_goothuis-oudshoorn_2011, @azur_et-al_2011]. The recommendation is that you elect to go with more iterations if not constrained by computational limitations.]. 

The advantage of this chained equation procedure is to estimate each variable as an outcome with its own regression model that is most appropriate for it. The regression models that one may use in `MICE` are as numerous as those a researcher may choose from when engaged in statistical analysis. This means that the assumptions and the performance of the model one uses for the imputation are the same as in standard statistical analyses. Though it decreases some of the requirements for modeling the MAR process, it is not entirely atheoretical. We can, however, reduce the dependence that the imputed data have on a researcher's ability to theorize about the MAR process by selecting models that are accustomed to dealing with a large number of parameters without increasing inefficiency.

One valuable model for allowing one to include a large number of parameters without losses to efficiency, is a form of ensemble machine learning model called Random Forests. As the following discussion highlights, random forest models are optimal for engaging in predictive tasks, which appears appropriate for the task of predicting missing values. These models have the additional benefit of not requiring a multivariate normal distribution, not requiring one to specify a potentially incorrect model of which variables are included in the MAR process, nor is it in the form of a Generalized Linear Model (GLM). That is, these models reduce some of the dependence of an imputation task on the researcher's beliefs about the source of the missingness. As researchers apply these random forest models within the `MICE` framework, they also benefit from the advantages that MI provide over the hot deck framework. These two features suggest that this procedure offers much more flexibility to the researcher. 

<!-- 
    END OF LIT REVIEW
-->
# The utility of random forest models for imputing political science data

Random forest models are concerned with calculating a fixed out-of-sample prediction under the supervised machine learning framework. They are popular among data scientists and researchers primarily interested in providing predictions instead of engaging in causal inference. In non-imputation applications of supervised machine learning models - a broader category of machine learning models that includes random forests, these models take partial, observed information about an outcome and estimate the relationship between the units with observed outcomes and several other observed features for those units. Then for the units without an observed outcome, we generalize that relationship to predict an outcome for them. 

To get an intuition of the fundamental goal of a supervised machine learning model, I will provide an example. If we are looking to sort beans into a good or bad pile before we toss them into a pot, we often want to collect information about them. Features like color, size, and plumpness can all be good indicators of whether a bean will taste good or bad. Say we have over 5,000 beans, and we are a chef at a restaurant approaching the dinner hour, and we do not have time to sort all these beans. To save time, we look at these features like color, size, and plumpness and make two piles - good or bad for a subset of our 5,000 beans; say, in this instance, about 25%. We then get one of our employees to sort the rest of the beans for us. This employee may know less than us about what features matter more and how to identify a bean that will taste good or bad. Nevertheless, they can look at the two piles we have already made and try to pick up on patterns that make good and bad beans different from one another. With this information, the employee can "learn" these patterns so that even without the same knowledge as the chef, they can still make predictions about whether a bean will taste good or bad.

We have some units with recorded observations for a particular variable for imputation. Leveraging this, we can treat these documented observations as information so that we can "train" our computer to find a relationship between the observed information in that variable and information from other variables for that unit. We can then generalize this relationship to predict what that value would be for a missing unit. This seems like a reasonable approach to thinking about imputing missing values. We are not making naive imputations by taking the mean value. As we are using an expanded form of MI, we can also use all variables in the dataset to clarify that pattern. As a `MICE` procedure, we can specify a machine learning model, and we do this iteratively so that if we have more than one variable that we want to impute, we are not limited to accurate imputed values only for the units that are complete except for that particular variable to be imputed. I will elaborate more on how this works in a few paragraphs. Before we go there, however, I want to take the time to provide a few more details about how random forest models work, as they do not represent the whole of supervised machine learning.

Practitioners refer to random forests as tree-based ensemble models. As a supervised machine learning model, they start with the basic intuition described above; but in the process of "learning" or "training," these models follow a few distinct steps. Decision tree models split regions of the predictive space. For example, say we want to predict vote choice by one's partisanship. We would split the predictive space into regions. These regions could be different degrees of partisanship, like strong Republicans and weak Republicans. When we split this predictive space into regions, we essentially are subsetting our dataset of partisans into these different areas of the predictive space and trying to optimize a model to provide the best predictions in each region in our predictive space. That is, we try to find a model that maximizes the predictive performance of vote choice for strong Republicans, weak Republicans, independents, and so on. We examine performance by comparing how well we are predicting the unobserved outcomes relative to the observed outcomes within those predictive regions. We can "bag" our trees. When we "bag" the decision trees, we bootstrap the training sample and build a tree for each bootstrapped sample and average across them. Doing this allows for a decrease in the variance of the predictions coming from the model, which helps with reducing the chances of generating a trained model that will fail to adequately generalize to our data set not included in the training step. 

As this is a primer for the applied researcher, I note that this discussion simplifies decision trees and random forest models. For those interested in more details about these concepts, @james_et-al_2013 provide a helpful discussion of these concepts. The main point is that random forest models specialize in generating predictions by optimizing at the value and not the variable level. This characteristic suggests that these models have the potential to be powerful tools for many different applications.

We can discuss their applicability to imputation in the `MICE` framework with a foundation in how random forest models work. As random forest models specialize in generating fixed out-of-sample predictions, these models have a joint goal with multiple imputation in that it should not be evaluated on the model's correctness but on the model's ability to predict a fixed (true) value [@rubin_1996]. Here, one might think of missing data as the out-of-sample predictions intended to be estimated. With random forest models, you train the model on a training data set (often randomly generated through cross-validation), a randomly selected portion of your data that you train the model on, and then fit the model on the testing set, the remaining data not used for the training stage of your model [@hastie_et-al_2009]. 

OLS models often perform relatively poorly on making out-of-sample predictions as they are BLUE, assuming the data on hand are relatively representative of the population. As we have MAR data, this assumption likely fails, and any out-of-sample predictions are likely to be biased. Further, OLS may also generate out-of-bounds predictions for non-continuous data [@long_1997, @gelman_et-al_2021] which is particularly troublesome in settings of prediction.

Other models provide within-bounds predictions, such as logistic models; however, as generalized linear models, they still assume a linear functional form. Though OLS and Logistic regression underlie a lot of machine learning as tools, many consider them to have limited applicability to complicated settings requiring prediction. Random forest models provide within-bounds predictions, and they are fully non-parametric [@hastie_et-al_2009], meaning they do not assume a functional form and consequently a joint distribution. This means that we have more flexibility in terms of what variables we have in our datasets that we need to impute. As social scientists, we rarely have datasets that contain DGPs that fall neatly within the optimal realm for GLMs. This added flexibility by using `MICE` and random forests makes the researcher's job easier. 

On other performance metrics, random forest models, as an ensemble method, provide much more accurate predictions than single-tree alternatives in machine learning, such as CART [@montgomery_olivella_2018]. As discussed above, rather than generating a single estimate from a single model, ensemble models, like random forests, calculate multiple models and learn from their performance; this is the purpose of using the bootstrapped samples.

In the context of using `MICE`, I argue that political scientists should consider using random forest models to make accurate predictions with fewer assumptions and be more lenient in terms of conditioning on the cause of MAR in the data set. Recall that within each `MICE` iteration, one performs a model predicting (imputing) a missing value based on the other variables in the dataset. Explicitly, this means you are running a model per variable with missing data. 

Given that random forests are non-parametric, within each `MICE` iteration, the relationship between the variable to be imputed and those used to make the predictions can be non-linear and can take many different multivariate distributional forms. This is a significant advancement on traditional MI, which assumes a multivariate normal distribution. Additionally, this is an advancement on other `MICE` models that political scientists use, which may not inherently conceptualize missing data as these unobserved values to predict from a generalized relationship of the observed variable with the other variables in the dataset. These relationships are also not assumed to be linear. Furthermore, using `RF-MICE` has the advantage over hot-deck procedures for those unsure about a variable's precise DGP for MAR. 

In the next section, I illustrate the use of random forest models for political scientists by demonstrating a simulated and real-world application of a random forest implementation of `MICE`. The following section also compares this implementation's performance to other common approaches to handling missing data in political science.
<!--
    END OF KEY SECTION FOR ARGUMENT
-->
# Application of `MICE` with random forests

## Simulated Data

```{python}
#| label: setup-block

# import modules
    #* from env
from itertools import repeat
from timeit import default_timer
import sys
import numpy as np
import pandas as pd
import duckdb as db
    #* user defined
sys.path.append('../code/')
from fun import simulate, ampute, impute, create_iter_table
#from utils import *
# Seed
np.random.seed(90210)
# Database
engine=db.connect("../data/sim_original.db")
engine2=db.connect("../data/sim_amputed.db")
engine3=db.connect("../data/sim_imputed.db")
engine4=db.connect('../data/benchmarks.db')
# Loop parameters
n: int=500
datasets: int=1000
m: int=10
```

I simulate two datasets to provide examples of simple scenarios. As we often use datasets we believe to contain causal relationships; I first consider two types of data generating processes when creating my simulated data. Using the `fabricatr` R package by @blair_et-al_2022, I first generate a dataset with a simple DGP depicted in the following equation: 

$$
Y = 0.9X + 0.3Z + \epsilon ~ N(0, 1)
$$

The parameters for X and Z are 0.9 and 0.3, respectively. I also include some random noise in the data generating process. For the dataset, the outcome variable, Y, is generated with the respective DGP and a link function that draws values, according to the DGP and breaks them into a 5-point ordinal scale from 1 to 5. In the dataset, I include nine other variables that are not part of the DGP for Y. As Multiple Imputation is often dependent on the amount of information included in the dataset, including more variables than just the predictors will allow for a test of the different procedures' success at including relevant information from the dataset in performing the imputation. I generate values for X and Z by using the values of the nine random variables. X is a continuos variable and its DGP is: 
$$ 
X = 0.5\text{Var2} + 0.3\text{Var6} + \epsilon ~ N(0, 1)
$$

Z is a binary variable where the probability, specifically 40%, of a value of 1 occurring is dependent on whether $Var5 = 1$, $Var7 = 0$, and $Var9 = 1$.

```{python}
#| label: simulated-generation-block

# Create 1000 datasets and store it in a list
sim = list(repeat(simulate(n), datasets))

# Take the original simulated datasets and store them in the database
for i in range(datasets):
    data_frame = sim[i]
    create_iter_table(data_frame = data_frame, name_append = "original", i = i, engine = engine)

# Once done, close the connection to the database
engine.close()
```

<!--
```{r}
#| label: simulated-generation-block
#| eval: false
# Simulate the data
    #* specify empty list to store simulated data in
simple_dgp_list = list()
    #* run a for-loop to simulate 1000 datasets with the specifications from the text
for(i in 1:datasets){
    simple_dgp = fabricate(N = n,
        Var1 = rnorm(N, mean = 10),
        Var2 = draw_binary(N = N, prob = 0.5),
        Var3 = draw_ordered(x = rnorm(N, mean = 3.5), breaks = c(1, 2, 3, 4, 5, 6, 7)),
        Var4 = rnorm(N, mean = 0),
        Var5 = draw_binary(N = N, prob = 0.25),
        Var6 = draw_ordered(x = rnorm(N, mean = 1.75), breaks = c(1, 2, 3, 4, 5, 6, 7)),
        Var7 = rnorm(N, mean = -10),
        Var8 = draw_binary(N = N, prob = 0.75),
        Var9 = draw_ordered(x = rnorm(N, mean = 5.25), breaks = c(1, 2, 3, 4, 5, 6, 7)),
        X = rnorm(n = N, mean = 0.5 * Var2 + 0.3 * Var6 + rnorm(N, 0)),
        Z = draw_binary(N = N, prob = ifelse(Var5 == 1 & Var7 == 0 & Var9 == 1, 0.4, 0.6)),
        Y = draw_ordered(x = (mean = 0.9 * X + 0.3 * Z + rnorm(N, 0)), breaks = c(1, 2, 3, 4))
    )
    name = paste('dataset', i, sep = '_')
    simple_dgp_list[[name]] = simple_dgp
}
save(simple_dgp_list, file="../data/original_datasets.RData")

```
-->

Once I have generated the dataset, I ampute it using the `mice` R package [@buuren_goothuis-oudshoorn_2011]. Following the estimate of the proportion of missing data in political science studies provided by @king_et-al_2001, I set the baseline proportion of values in the dataset to be imputed to 0.4. The amputation procedure is rather simple where the DGP for the missingness is determined through a linear regression. As the `RF-MICE` procedure performs best under complicated MAR patterns -- particularly those more complicated than a linear regression, I expect that this simulation will yield results quite similar to that of `AMELIA II` and other parametric procedures. This would mean that, for this particular circumstance, that `RF-MICE` performs no worse than `AMELIA II`; but for all the reasons above, we should expect `RF-MICE` to perform well under settings with more complicated missing data patterns.

```{python}
#| label: amputation-block

# Take each of the dataframes in the sim object, ampute them, and return a list object containing all of the dataframes
amputed = list(map(lambda x: ampute(x, p_miss=0.4, p_obs=0.5, mecha="MAR"), sim))
# Store results in database
for i in range(datasets):
    data_frame = amputed[i]
    create_iter_table(data_frame = data_frame, name_append = "amputed", engine = engine2, i = i)

# Close connection to database   
engine2.close()
```

<!--
SEE HOW R DOES ON ITS OWN
```{python}
#| label: python-impute
#| eval: false
start_time = default_timer()
completed_df = list(map(lambda x: impute(data_frame=x, datasets=datasets), amputed))
total_time = default_timer() - start_time
# Store results in database
for i in range(datasets):
    data_frame = completed_df[i]
    create_iter_table(data_frame = data_frame, name_append = "python_imputed", engine = engine3, i = i)

# Close connection to database
engine3.close()
```

```{r}
#| label: simulated-amputation-block
#| eval: false
# ampute the data
    #* specify empty list to store amputed datasets in 
simple_amputed_list = list()
    #* specify the weights used for the amputation
ID = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
Var1 = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
Var2 = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
Var3 = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
Var4 = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
Var5 = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
Var6 = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
Var7 = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
Var8 = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
Var9 = c(0, 0, 0.072, 0, 0.072, 0, 0, 0, 0, 0, 0.072, 0.072, 0.072)
X = c(0, 0, 0.076, 0, 0.076, 0, 0, 0, 0, 0, 0.076, 0.076, 0.076)
Y = c(0, 0, 0.076, 0, 0.076, 0, 0, 0, 0, 0, 0.076, 0.076, 0.076)
Z = c(0, 0, 0.076, 0, 0.076, 0, 0, 0, 0, 0, 0.076, 0.076, 0.076)
weights = cbind(ID, Var1, Var2, Var3, Var4, Var5, Var6, Var7, Var8, Var9, X, Y, Z)
    #* specify proportion of missingness
missing_prop = c(0.1, 0.4, 0.9)
    #* run a for-loop to ampute each of the 1000 datasets with the specifications from the text

for(d in 1:data_sets){
    for(i in missing_prop){
        x = ampute(simple_dgp_list[[d]], mech = 'MAR', prop = i, freq = c(0, 0, 0.175, 0, 0, 0.175, 0, 0, 0, 0, 0.175, 0.175, 0.3), weights = weights)
        assign(paste0('simple_mar_', i), x)
    }
    name = paste('amputed', d, sep = '_')
    simple_amputed_list[[name]] = list('simple_mar_0.1' = simple_mar_0.1, 'simple_mar_0.4' = simple_mar_0.4, 'simple_mar_0.9' = simple_mar_0.9)
}
saveRDS(simple_amputed_list, '../simple_amputed_list.RDS')
```
-->
With these amputed datasets, I then apply some possible procedures one can use to impute these values. Interpolation is a quite simple procedure where I can fill in missing values by using the mean value of that particular variable. I perform this interpolation with the `MICE` package and iterate over it to provide 10 datasets. I also use the `AMELIA II` package [@honaker_et-al_2011] to perform standard MI and also store 10 datasets from the iterations. I use a standard Bayesian linear model in the `MICE` framework. Bayesian linear models with uniform distributions or a weak prior distribution are similar to the familiar Ordinary Least Squares [@gelman_et-al_2021]. As discussed before, the final procedure I use is a random forest in the `MICE` framework.

```{r}
#| label: simulation-r-setup-block
# Set seed  
set.seed(90210)
# Modularly load required functions
box::use(
    DBI = DBI[dbConnect, dbGetQuery, dbWriteTable],
    duckdb = duckdb[duckdb], #needs to be version 0.6.1 for both R and python
    mice = mice[ampute, pool, complete, mice],
    miceRanger = miceRanger[miceRanger],
    data.table = data.table[data.table],
    Amelia = Amelia[amelia],
    tictoc = tictoc[...],
    dplyr = dplyr[select, filter, bind_rows],
    tibble = tibble[tibble],
    ggplot2 = ggplot2[ggplot, geom_density, aes, theme_minimal, labs, geom_boxplot, ggsave],
)

# Load user defined functions
source("../code/fun.R")
# Loop parameters
n <- 500 # size of each sample
datasets <- 1000 # size of number of samples

# Connect to databases
engine <- dbConnect(duckdb(), dbdir = "../data/sim_original.db", read_only = FALSE)
engine2 <- dbConnect(duckdb(), dbdir = "../data/sim_amputed.db", read_only = FALSE)
engine3 <- dbConnect(duckdb(), dbdir = "../data/sim_imputed.db", read_only = FALSE)
engine4 <- dbConnect(duckdb(), dbdir = "../data/benchmarks.db")
```

```{r}
#| label: load-simulated-data-r
original <- list()
amputed <- list()

for (i in 0: 999) {
    original_table <- paste0('original_', as.character(i), sep="")
    original[[original_table]] <- dbGetQuery(engine, paste0('SELECT * FROM ', original_table, sep=""))
}

for (i in 0: 999) {
    amputed_table <- paste0('amputed_', as.character(i), sep="")
    df <- dbGetQuery(engine2, paste0('SELECT * FROM ', amputed_table, sep=""))
    amputed[[amputed_table]] <- df
}
```

<!--
    Notes:
        - The following block has the option `eval:false` set.
        - Remove this option if wanting to reproduce the whole thing.
        - Otherwise, can just access them from the database like I do in the following block
-->

```{r}
#| label: r-imputations
#| eval: false
# Interpolation
tic.clearlog()
tic("Interpolate")
interpolate <- impute(package="mice", meth="mean")
toc(log = TRUE, quiet = TRUE)

# AMELIA
tic("Amelia")
amelia <- impute(package="amelia", con = engine4)
toc(log = TRUE, quiet = TRUE)

# LMICE
tic("LMICE")
lmice <- impute(package="mice", meth="norm", con = engine4)
#lmice <- lapply(amputed, mice, c(m = 10, meth = "norm"))
toc(log = TRUE, quiet = TRUE)

# RFMICE
tic("RFMICE")
rfmice <- impute(package="mice", meth="rf", con = engine4)
#rfmice <- lapply(amputed, mice, c(m = 10, meth = "rf"))
toc(log = TRUE, quiet = TRUE)

#RFRanger
tic("RFRanger")
rfranger <- impute(package="miceRanger", con = engine4)
#rfranger <- lapply(amputed, miceRanger, c(m = 10, verbose = FALSE))
toc(log = TRUE, quiet = TRUE)

#Store timing info
log <- tictoc::tic.log(format = TRUE) |>
    stringr::str_split( ": ", simplify = TRUE) |>
    data.frame()
log$X2<-as.double(gsub("[secelapsed]","",as.character(log$X2)))
setnames(log, c("procedure","seconds"))
dbWriteTable(engine4, "benchmark", log, overwrite = TRUE)

# Store imputed data in database
for (i in 1: datasets) {
    df <- interpolate[[i]]
    dbWriteTable(engine3, paste0("mean_", as.character(i)), df, overwrite = TRUE)
}
for (i in 1: datasets) {
    df <- amelia[[i]]
    dbWriteTable(engine3, paste0("amelia_", as.character(i)), df, overwrite = TRUE)
}
for (i in 1: datasets) {
    df <- lmice[[i]]
    dbWriteTable(engine3, paste0("lmice_", as.character(i)), df,overwrite = TRUE)
}
for (i in 1: datasets) {
    df <- rfmice[[i]]
    dbWriteTable(engine3, paste0("rfmice_", as.character(i)), df,overwrite = TRUE)
}
for (i in 1: datasets) {
    df <- data.frame(rfranger[[i]])
    dbWriteTable(engine3, paste0("rfranger_", as.character(i)), df,overwrite = TRUE)
}
```

<!--
    Notes:
        - Change `eval:true` to `eval:false` in the following block if fully reproducing the manuscript
        - leave to `eval:true` if not re-running simulations
-->
```{r}
#| label: simulated-imputation-load
# Create empty list elements
interpolate <- list()
amelia <- list()
lmice <- list()
rfmice <- list()
rfranger <- list()

# Grab the stored imputed data and put each dataframe in list
for (i in 1: datasets) {
    mean_table <- paste0('mean_', as.character(i), sep="")
    df <- dbGetQuery(engine3, paste0('SELECT * FROM ', mean_table, sep=""))
    interpolate[[mean_table]] <- df
}

for (i in 1: datasets) {
    amelia_table <- paste0('amelia_', as.character(i), sep="")
    df <- dbGetQuery(engine3, paste0('SELECT * FROM ', amelia_table, sep=""))
    amelia[[amelia_table]] <- df
}

for (i in 1: datasets) {
    lmice_table <- paste0('lmice_', as.character(i), sep="")
    df <- dbGetQuery(engine3, paste0('SELECT * FROM ', lmice_table, sep=""))
    lmice[[lmice_table]] <- df
}

for (i in 1: datasets) {
    rfmice_table <- paste0('rfmice_', as.character(i), sep="")
    df <- dbGetQuery(engine3, paste0('SELECT * FROM ', rfmice_table, sep=""))
    rfmice[[rfmice_table]] <- df
}

for (i in 1: datasets) {
    rfranger_table <- paste0('rfranger_', as.character(i), sep="")
    df <- dbGetQuery(engine3, paste0('SELECT * FROM ', rfranger_table, sep=""))
    rfranger[[rfranger_table]] <- df %>%
        tidyr::gather(
            key = "dataset", 
            value = "A",
            Dataset_1.A,
            Dataset_2.A,
            Dataset_3.A,
            Dataset_4.A,
            Dataset_5.A,
            Dataset_6.A,
            Dataset_7.A,
            Dataset_8.A,
            Dataset_9.A,
            Dataset_10.A) %>%
        tidyr::gather(
            key = "dataset",
            value = "B",
            Dataset_1.B,
            Dataset_2.B,
            Dataset_3.B,
            Dataset_4.B,
            Dataset_5.B,
            Dataset_6.B,
            Dataset_7.B,
            Dataset_8.B,
            Dataset_9.B,
            Dataset_10.B
        ) %>%
        tidyr::gather(
            key = "dataset",
            value = "X",
            Dataset_1.X,
            Dataset_2.X,
            Dataset_3.X,
            Dataset_4.X,
            Dataset_5.X,
            Dataset_6.X,
            Dataset_7.X,
            Dataset_8.X,
            Dataset_9.X,
            Dataset_10.X
        ) %>%
        tidyr::gather(
            key = "dataset",
            value = "Y",
            Dataset_1.Y,
            Dataset_2.Y,
            Dataset_3.Y,
            Dataset_4.Y,
            Dataset_5.Y,
            Dataset_6.Y,
            Dataset_7.Y,
            Dataset_8.Y,
            Dataset_9.Y,
            Dataset_10.Y
        )
}
```
<!--
```{r}
#| label: simulated-imputation-block
#| results: hide
#| eval: false
# Impute the missing data
    #* reformat list objects
simple_mar_0.1_list = list()
simple_mar_0.4_list = list()
simple_mar_0.9_list = list()
for(d in 1:data_sets){
    simple_mar_0.1_list[[d]] = simple_amputed_list[[d]][['simple_mar_0.1']]
    simple_mar_0.4_list[[d]] = simple_amputed_list[[d]][['simple_mar_0.4']]
    simple_mar_0.9_list[[d]] = simple_amputed_list[[d]][['simple_mar_0.9']]
}
    #* interpolation
simple_mean_list_0.4 = list()
for(d in 1:data_sets){
    simple_mean = mice(simple_mar_0.4_list[[d]]$amp, m = 10, meth = 'mean')
    simple_mean_list_0.4[[d]] = simple_mean
}
saveRDS(simple_mean_list_0.4, '../simple_mean_list_04.RDS')

simple_mean_list_0.1 = list()
for(d in 1:data_sets){
    simple_mean = mice(simple_mar_0.1_list[[d]]$amp, m = 10, meth = 'mean')
    simple_mean_list_0.1[[d]] = simple_mean
}
simple_mean_list_0.9 = list()
for(d in 1:data_sets){
    simple_mean = mice(simple_mar_0.9_list[[d]]$amp, m = 10, meth = 'mean')
    simple_mean_list_0.9[[d]] = simple_mean
}
    #* AMELIA
simple_amelia_list_0.4 = list()
for(d in 1:data_sets){
    simple_amelia = amelia(data.frame(simple_mar_0.4_list[[d]]$amp), m = 10)
    simple_amelia_list_0.4[[d]] = simple_amelia
}
saveRDS(simple_amelia_list_0.4, '../simple_amelia_list_04.RDS')

simple_amelia_list_0.1 = list()
for(d in 1:data_sets){
    simple_amelia = amelia(data.frame(simple_mar_0.1_list[[d]]$amp), m = 10)
    simple_amelia_list_0.1[[d]] = simple_amelia
}
simple_amelia_list_0.9 = list()
for(d in 1:data_sets){
    simple_amelia = amelia(data.frame(simple_mar_0.9_list[[d]]$amp), m = 10)
    simple_amelia_list_0.9[[d]] = simple_amelia
}
    #* linear bayes
simple_linear_list_0.4 = list()
for(d in 1:data_sets){
    simple_linear = mice(simple_mar_0.4_list[[d]]$amp, m = 10, meth = 'norm')
    simple_linear_list_0.4[[d]] = simple_linear
}
saveRDS(simple_linear_list_0.4, '../simple_linear_list_04.RDS')

simple_linear_list_0.1 = list()
for(d in 1:data_sets){
    simple_linear = mice(simple_mar_0.1_list[[d]]$amp, m = 10, meth = 'norm')
    simple_linear_list_0.1[[d]] = simple_linear
}
simple_linear_list_0.9 = list()
for(d in 1:data_sets){
    simple_linear = mice(simple_mar_0.9_list[[d]]$amp, m = 10, meth = 'norm')
    simple_linear_list_0.9[[d]] = simple_linear
}
    #* RF-MICE
simple_rf_list_0.4 = list()
for(d in 1:data_sets){
    simple_rf = mice(simple_mar_0.4_list[[d]]$amp, m = 10, meth = 'rf')
    simple_rf_list_0.4[[d]] = simple_rf
}
saveRDS(simple_rf_list_0.4, '../simple_rf_list_04.RDS')

simple_rf_list_0.1 = list()
for(d in 1:data_sets){
    simple_rf = mice(simple_mar_0.1_list[[d]]$amp, m = 10, meth = 'rf')
    simple_rf_list_0.1[[d]] = simple_rf
}
simple_rf_list_0.9 = list()
for(d in 1:data_sets){
    simple_rf = mice(simple_mar_0.9_list[[d]]$amp, m = 10, meth = 'rf')
    simple_rf_list_0.9[[d]] = simple_rf
}
```
-->
<!--
Figure \ref{fig:distributions} depicts the distributions of the complete dataset (solid black line), the dataset as MAR (dotted black line), and the ten datasets generated from the given procedure (solid line with varying hues of grey) for the outcome variable, Y, in the simple DGP dataset. As expected, AMELIA and Linear Regression in MICE provide out-of-sample predictions. Relative to interpolation, the RF-MICE procedure does a better job replicating the original distribution where the black line is covered by the lines from the imputed datasets.
-->

Each imputation procedure produced $m = 10$ datasets per simulated dataset, $s = 1000$. I have a total of $s \times m$ datasets. For each s dataset, I took the difference of each m dataset from the complete dataset and took the average of these differences to give me a mean score of the discrepancy for each s dataset. @fig-y-simulated, @fig-x-simulated, @fig-z-simulated represents these mean discrepancy scores for the three variables that were originally imputed.

```{r}
#| label: calculating average discrepancies

# boxplots of difference between values
    #* Take the mean difference within imputed datasets
        #** Mean
interpolate_diff = discrepancy(imputed = interpolate, original = original)
        #** Amelia
amelia_diff = discrepancy(imputed = amelia, original = original)
        #** LMice
lmice_diff = discrepancy(imputed = lmice, original = original)
        #** RFMice
rfmice_diff = discrepancy(imputed = rfmice, original = original)
        #** RFRanger
rfranger_diff = discrepancy(imputed = rfranger, original = original)
```

```{r}
#| label: fig-y-simulated
#| eval: false
#| fig-cap: Average discrepancy scores for Y
ggplot() +
    geom_boxplot(aes(x = 'Mean', y = Y), data = mean_mean_diff) +
    geom_boxplot(aes(x = 'AMELIA', y = Y), data = mean_amelia_diff) +
    geom_boxplot(aes(x = 'Linear MICE', y = Y), data = mean_linear_diff) +
    geom_boxplot(aes(x = 'RF MICE', y = Y), data = mean_rf_diff) +
    theme_minimal() +
    labs(x = 'Y', y = 'Average difference from complete data')
```

```{r}
#| label: fig-x-simulated
#| eval: false
#| fig-cap: Average discrepancy scores for X
ggplot() +
    geom_boxplot(aes(x = 'Mean', y = X), data = mean_mean_diff) +
    geom_boxplot(aes(x = 'AMELIA', y = X), data = mean_amelia_diff) +
    geom_boxplot(aes(x = 'Linear MICE', y = X), data = mean_linear_diff) +
    geom_boxplot(aes(x = 'RF MICE', y = X), data = mean_rf_diff) +
    theme_minimal() +
    labs(x = 'X', y = 'Average difference from complete data')
```

```{r}
#| label: fig-z-simulated
#| eval: false
#| fig-cap: Average discrepancy scores for Z
ggplot() +
    geom_boxplot(aes(x = 'Mean', y = Z), data = mean_mean_diff) +
    geom_boxplot(aes(x = 'AMELIA', y = Z), data = mean_amelia_diff) +
    geom_boxplot(aes(x = 'Linear MICE', y = Z), data = mean_linear_diff) +
    geom_boxplot(aes(x = 'RF MICE', y = Z), data = mean_rf_diff) +
    theme_minimal() +
    labs(x = 'Z', y = 'Average difference from complete data')
```

Overall, we see that the procedures yield small differences between the actual and estimated values. For Y, `AMELIA II` has a mean discrepancy of ```r sprintf('%.3f', mean(mean_amelia_diff$Y)) ```, the Linear `MICE` procedure has a mean discrepancy of ```r sprintf('%.3f', mean(mean_linear_diff$Y)) ```, the interpolation procedure has a mean discrepancy of ```r sprintf('%.3f', mean(mean_mean_diff$Y)) ```, and `RF-MICE` has a mean discrepancy of ```r sprintf('%.3f', mean(mean_rf_diff$Y)) ```. For X, we see that the `RF-MICE` has a mean discrepancy of ```r sprintf('%.3f', mean(mean_rf_diff$X)) ```, interpolation has a mean discrepancy of ```r sprintf('%.3f', mean(mean_mean_diff$X)) ```, Linear `MICE` has a mean discrepancy of ```r sprintf('%.3f', mean(mean_linear_diff$X)) ```, and AMELIA has a mean discrepancy of ```r sprintf('%.3f', mean(mean_amelia_diff$X)) ```. For Z, we also see that `RF-MICE` has a mean discrepancy of ```r sprintf('%.3f', mean(mean_rf_diff$Z)) ```, interpolation has a mean discrepancy of ```r sprintf('%.3f', mean(mean_mean_diff$Z)) ```, Linear `MICE` has a mean discrepancy of ```r sprintf('%.3f', mean(mean_linear_diff$Z)) ``` and `AMELIA II` has a mean discrepancy of ```r sprintf('%.3f', mean(mean_amelia_diff$Z)) ```. 

The differences between the procedures across all three variables are similar. This may be due to the fact that the simulation and the amputation is relatively simple to what we may encounter in the real world where we expect the MAR process to be much more complicated. This demonstrates, however, that the performance of these procedures are relatively heterogenous and that, for robustness' sake, we should perform our imputation using more than one tool. Speaking directly to the `RF-MICE` procedure, we see that it performs well for a dichotomous (one-hot encoded) outcome but performs less well than `AMELIA II` in circumstances where the outcome is continuous or ordinal. In a separate simulation study, @marbach_2021_pa also demonstrates similar differences in performance between `AMELIA II` and `RF-MICE` where the MAR procedure does not include an interaction term.^[When the MAR process includes an interaction term, `RF-MICE` tends to underperform relative to `AMELIA II`.]

## Simple applied example

In this section, I use the 2020 American National Election Study to provide a simple example of the steps one might take to handle missing data and how they might implement the `RF-MICE` procedure in practice. The example that I use here is to examine the predictors of vote choice among the American electorate. While this is a rich area of research, I use an admittedly oversimplified version of a model of vote choice. In the supplementary materials, readers may find code to replicate the steps I take to do this.
```{r}
#| label: anes-cleaning
#| eval: false
# Title: 2020 ANES applied example 

# Setup 
    #* set the seed
set.seed(90210)
    #* modularly load the functions that I'll need for the applied example
box::use(
    haven = haven[read_dta],
    dplyr = dplyr[select, mutate_all, mutate, rename],
    finalfit = finalfit[missing_pairs, missing_compare],
    ggplot2 = ggplot2[labs],
    kableExtra = kableExtra[kbl],
    Amelia = Amelia[amelia],
    mice = mice[mice],
    miceadds = miceadds[datalist2mids],
    missDiag = missDiag[missDiag],
    ggplot2 = ggplot2[ggplot, geom_boxplot, aes, theme_minimal, labs]
)
    #* create empty anes_2020 list object
anes_2020 = list()
    #* load the dataset and store it in the anes_2020 list object as original
anes_2020[['original']] = read.csv('data/anes_timeseries_2020/anes_time_series_2020.csv')

# Cleaning
anes_2020[['clean']] = anes_2020[['original']] |>
    select('V202054x', 'V202065x', 'V202068x', 'V202105x', 'V202106x', 'V202107x', 'V202108x', 'V202116', 'V202117', 'V202118', 'V202119', 'V202122', 'V202123', 'V202138y', 'V202139y1', 'V202140y1', 'V202141y1', 'V202142y1', 'V202143', 'V202144', 'V202145', 'V202146', 'V202147', 'V202148', 'V202149', 'V202150', 'V202156', 'V202157', 'V202158', 'V202159', 'V202160', 'V202161', 'V202162', 'V202163', 'V202164', 'V202165', 'V202166', 'V202167', 'V202168', 'V202169', 'V202170', 'V202171', 'V202172', 'V202173', 'V202174', 'V202175', 'V202176', 'V202177', 'V202178', 'V202179', 'V202180', 'V202181', 'V202182', 'V202183', 'V202184', 'V202185', 'V202186', 'V202187', 'V202203x', 'V202212', 'V202213', 'V202214', 'V202215', 'V202216', 'V202219', 'V202220', 'V202221', 'V202222', 'V202223', 'V202224', 'V202225', 'V202231x', 'V202232', 'V202233', 'V202236x', 'V202242x', 'V202252x', 'V202255x', 'V202259x', 'V202260', 'V202261', 'V202262', 'V202263', 'V202264', 'V202265', 'V202266', 'V202267', 'V202268', 'V202269', 'V202270', 'V202273x', 'V202276x', 'V202279x', 'V202282x', 'V202286x', 'V202290x', 'V202292', 'V202300', 'V202301', 'V202302', 'V202303', 'V202304', 'V202305', 'V202308x', 'V202310', 'V202311', 'V202312', 'V202320x', 'V202321', 'V202325', 'V202328x', 'V202331x', 'V202336x', 'V202341x', 'V202344x', 'V202347x', 'V202350x', 'V202351', 'V202352', 'V202355', 'V202357', 'V202361x', 'V202364x', 'V202367x', 'V202370x', 'V202373x', 'V202376x', 'V202377', 'V202380x', 'V202383x', 'V202387x', 'V202390x', 'V202451', 'V202457', 'V202468x', 'V202490x', 'V202493x', 'V201507x', 'V201511x', 'V201533x', 'V201549x', 'V201600') |>
    mutate_all(~ifelse(.x >= 0, .x, NA)) |> 
    mutate(`White` = ifelse(V201549x == 1, 1, 0),
            `Female` = ifelse(V201600 == 2, 1, 0),
            `PID` = ifelse(V202065x == 1, 1,
                            ifelse(V202065x > 3, 2, 3)),
            `Vote` = ifelse(V202105x == 10, 1, 0),
            `Age` = V201507x,
            `Employment` = V201533x,
            `Education` = V201511x) #|>
    #select(White, Female, PID, Vote, Age, Employment, Education)
```

ANES conducts the survey in two waves; the first wave occurred in the months leading up to the 2020 Presidential Election, and the post wave happened in the months after. As there are two waves, there is ample opportunity for multicollinearity to occur and to cause both imputation procedures some problems. As a result, I removed the pre-election survey items and a portion of the post-election survey items (primarily those that are administrative or are variables later used to create summary responses). This cleaning of the dataset leaves ```r ncol(anes_2020[['clean']]) ``` variables.

Before imputation I also recoded the variables that will be used in the statistical model. Vote is coded as 1 if the respondent reported voting for Joe Biden, and 0 if they did not. The variable White is coded as 1 if the respondent self-reported that they are White and 0 otherwise. The variable Female is coded as 1 if the respondent self-reported that they are Female and 0 otherwise. Finally, the variable Party ID is a 3-item measure where respondents that reported that they are Democrats are coded as a 1, those that reported that they are independent are coded as 2 and those that reported that they are Republicans are coded as 3. I also will use the default coding that the ANES provides for the respondent's age, employment status, and their education.

Before I impute the data, I examine the patterns of missingness in the dataset. I am particularly interested in examining these missingness patterns on the dependent variable of the statistical analysis I intend to perform later - an indicator variable of voting for the Democratic presidential candidate or not. What variables seem to be plausible causes of missingness in a MAR pattern are partisanship of the respondent, age, whether the respondent is white, and their level of education. Those who lost the election may be less likely to report how they voted in the election to a surveyor. I might also expect those who are less educated to be less likely to report how they voted in an election due to their proclivity to participate in surveys less and to vote at lower rates. I also expect that the race of the respondent and gender may affect the degree to which one is willing to respond. @fig-missing-pattern presents the a matrix demonstrating the relationship between missing and observed values with other variables.

```{r}
#| label: fig-missing-pattern
#| eval: false
#| fig-cap: Patterns of missing values

anes_2020[['clean']] |>
    mutate(`White` = factor(`White`, labels = c('N-W', 'W')),
            `PID` = factor(`PID`, labels = c('Dem', 'Ind', 'Rep')),
            `Vote` = factor(`Vote`, labels = c('Rep', 'Dem')),
            `Female` = factor(`Female`, labels = c('Male', 'Female')),
            `Education` = factor(`Education`),
            `Employment` = factor(`Employment`),
            `Age` = `Age`) |>
    missing_pairs(dependent = 'Vote', explanatory = c('PID', 'Female', 'White', 'Age', 'Education'), position = 'fill') + labs(Title = 'Missing Data Patterns', caption = 'Data Source: 2020 American National Election Study')
```

We do see slight differences in the mean values of the observed and missing observations. It is unclear, however, whether these differences are meaningful. To test whether these differences are meaningful, I again define Democratic vote share as the dependent variable and include the same explanatory variables: PID, Age, Education, White, and Female. I then perform a $\chi^2$ test to examine the difference between observed and unobserved values. The results of this test are included in @tbl-missing-pattern.

```{r}
#| label: tbl-missing-pattern
#| tbl-cap: Patterns of missing values
#| eval: false

missing_pattern_table = anes_2020[['clean']] |>
    mutate(`White` = factor(`White`, labels = c('N-W', 'W')),
        `PID` = factor(`PID`, labels = c('Dem', 'Ind', 'Rep')),
        `Vote` = factor(`Vote`, labels = c('Rep', 'Dem')),
        `Female` = factor(`Female`, labels = c('Male', 'Female')),
        `Education` = factor(`Education`),
        `Employment` = factor(`Employment`),
        `Age` = `Age`) |>
    missing_compare(dependent = 'Vote', explanatory = c('PID', 'Female', 'White', 'Age', 'Education', 'Employment'))
kbl(missing_pattern_table, row.names = FALSE, align = c('l', 'l', 'r', 'r', 'r'), booktabs = TRUE, format = 'latex')
```

The results suggest that missing values in Democratic Vote Share depend on the respondent's age, education level, and whether they identify as Non-White. This suggests that Democratic vote share, the dependent variable in our eventual statistical model, is MAR. Therefore, we should perform imputation to reduce bias. It is important to note that there are over 130 other variables in the dataset that may contribute to the missingness of Democratic vote share. We also see in @fig-fig-missing-pattern that the other variables might also be MAR. When one is considering performing imputation, one should examine these possibilities.

I again choose to take three approaches to deal with the missing data. The first is likely to produce more bias as we just identified that the outcome variable is likely MAR; this is LWD. The second is to use `AMELIA II` to perform multivariate normal MI. The third is the `RF-MICE` procedure. 

```{r}
#| label: anes-imputations
#| eval: false
    #* AMELIA
anes_amelia = amelia(anes_2020[['clean']], m = 10, parallel = "snow")
saveRDS(anes_amelia, '../data/anes_amelia_imputed.RDS')
    #* RF-MICE
anes_rf = mice(anes_2020[['clean']], m = 10, method = 'rf')
saveRDS(anes_rf, '../data/anes_rf_imputed.RDS')
```

```{r}
#| label: load-anes-imputations
#| eval: false
anes_amelia <- readRDS('../data/anes_amelia_imputed.RDS')
anes_rf <- readRDS('../data/anes_rf_imputed.RDS')
```

This is still a large dataset, and both imputation procedures will require patience from the researcher. Those with computational resource constraints may consider generating fewer datasets per iteration or by doing fewer iterations - though exact recommendations about how many iterations to perform vary between 3 and 10 with no concrete recommendation other than the closer you can get to doing 10, the better @buuren_goothuis-oudshoorn_2011. Here, I perform both procedures on the ANES and calculate 10 data sets.

I use the `missDiag` package [see @marbach_2021_pa] to estimate the possible discrepancy between my observed data and that which I imputed. The discrepancy statistics are estimated by treating a given variable as a possible covariate balancing problem where under MCAR, one would expect that the dataset is balanced for a given variable.

```{r}
#| label: fig-anes-discrepancy-y
#| fig-cap: Standardized mean difference of Vote
#| eval: false

smd_amelia = missDiag(original = anes_2020[['clean']], imputed = datalist2mids(anes_amelia[['imputations']]), formula = `Vote` ~.)
smd_rf = missDiag(original = anes_2020[['clean']], imputed = anes_rf, formula = `Vote` ~.)

ggplot() +
    geom_boxplot(aes(x = 'AMELIA', y = smd_amelia$diff_adj)) +
    geom_boxplot(aes(x = 'RF-MICE', y = smd_rf$diff_adj)) +
    theme_minimal() +
    labs(y = 'Standardized mean difference', caption = 'Data source: 2020 American National Election Study')
```

To examine how both procedures perform in terms of bias in statistical estimates relative to LWD and each other, I run a standard linear regression with the imputed datasets and the original dataset.

The outcome of the regression model is whether the individual voted for Joe Biden in the 2020 Presidential Election. Following the classic model of vote choice discussed by @campbell_et-al_1969, I include the respondent's partisanship (coded as 1 for Democrat, 2 for Moderate, and 3 for Republicans), their age, level of education, an indicator variable for female respondents, an indicator variable for white respondents, and included a measure of occupational status. 

While this is a very simple model of vote choice, this specification allows me to hold everything but the difference between the observations missing in the original data constant. From this model, as Joe Biden is a Democrat, we should expect that Democrats will be more likely to support Democrats (which would be represented, based on my coding scheme, as a negative relationship). More educated respondents will be more supportive of Joe Biden, female respondents would be more likely to report support for Joe Biden, and white respondents, holding partisanship constant, would be less likely to be supportive of Joe Biden due to the association of Trump with white grievance politics @Jardina2020b. 

With the independent variables we chose, we should expect relatively low levels of bias in LWD. As we tested for possible sources of MAR, we understand that these sources are also theoretically-relevant predictors of our dependent variable that I include in my statistical model. This should help us account for the causes of missingness in our dependent variable. This means that, on balance, the procedure that comes closer to LWD is likely to be a better-performing model than the alternative if the differences are large.

The results of these models are depicted in @tbl-regression.

```{r}
#| label: anes-models-block
#| eval: false
# Models
    #* LWD
lwd = lm(`Vote` ~ `PID` + `White` + `Female` + `Age` + `Education` + `Employment`, data = anes)
    #* AMELIA
amelia = pool(lapply(anes_amelia$imputations, function(x) lm((`Vote` ~ `PID` + `White` + `Female` + `Age` + `Education` + `Employment`, data = x))))
    #* RF-MICE
rf = pool(with(anes_rf_long, lm((`Vote` ~ `PID` + `White` + `Female` + `Age` + `Education` + `Employment`))))

mods = list(
    'Listwise Deletion' = lwd,
    'AMELIA' = amelia,
    'RF-MICE' = rf
)
cm = c(
    'PID' = 'PID',
    'White' = 'White',
    'Female' = 'Female',
    'Education' = 'Education',
    'Age' = 'Age',
    'Employment' = 'Employment',
    '(Intercept)' = 'Constant'
)
gm = list(
    list('raw' = 'nobs', 'clean' = 'N', fmt = 0),
    list('raw' = 'nimp', 'clean' = 'Imputations', fmt = 0),
    list('raw' = 'adj.r.squared', 'clean' = 'Adj. R$^2$', fmt = 2)
)
modelsummary(mods, coef_map = cm, gof_map = gm, notes = list('Data Source: 2020 American National Election Study.', 'Estimates from OLS.', 'Standard Errors in Parentheses.'), stars = c('*' = 0.05))
```
-->
<!--
    END OF METHODS SECTION
-->
<!--
    END OF DOCUMENT
-->