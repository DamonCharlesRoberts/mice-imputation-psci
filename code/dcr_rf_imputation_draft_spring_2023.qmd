---
title: |
  Giving the leaves back to the forest
subtitle: A primer on the use of random forest models as chained equations for imputing missing data
short-title: Giving the leaves back to the forest
code-repo: "Replication materials are stored at <https://github.com/DamonCharlesRoberts/mice-imputation-psci>"
author:
  - name: Damon C. Roberts
    email: damon.roberts-1@colorado.edu
    orcid: 0000-0002-4360-3675
    title: PhD Candidate
    affiliations:
      - id: CU
        name: University of Colorado Boulder
        department: Political Science
        address: 333 UCB
        city: Boulder
        region: CO 
        postal-code: 80309-0333
    attributes:
      corresponding: true   
abstract: |
    Political scientists often struggle with decisions about what to do with incomplete cases for their regression analyses, and one can often make several decisions that influence oneâ€™s ultimate substantive conclusions. In many areas of research outside of political science and the social sciences, scholars take advantage of an extension of multiple imputation, which offers the choice to leverage machine learning models for predicting values in missing data. This manuscript provides a summary of missing data and its consequences for our regression models along with providing an explanation of how to implement random forest models with an expanded form of the multiple imputation procedure, called multiple imputation with chained equation to handle complex causes for non-random missingness in our data. After providing a primer on standard missing data procedures in political science and random forest with multiple imputation with chained equations, I examine its performance on simulated data. I conclude by providing recommendations for dealing with missing data in practice. 
thanks: |
    I would like to thank Andrew Q. Philips, Jennifer Wolak, and Madeline Mader for their advice and many conversations during the development of this project as well as Andy Baker for offering me the space to write the manuscript and for his feedback. I would also like to thank the discussants and panelists at MPSA for their useful feedback and encouragement.
published: "Working paper. Please do not distribute without author consent."
keywords:
  - Missing data
  - Multiple imputation
  - Machine learning
date: today
bibliography: "../assets/references.bib"
nocite: |
    @dbi, @duckdb, @r, @dplyr, @itertools, @sys, @tibble,
format:
  hikmah-pdf:
    # Spit out in drafts directory
    latex-output-dir: "../drafts/"
    # Use biblatex-chicago
    biblatex-chicago: true
    biblio-style: authordate
    biblatexoptions: 
      - backend=biber
      - autolang=hyphen
      - isbn=false
      - uniquename=false
  #pdf:
  #  template: "../../../templates/manuscripts/apsr_template.tex"
  #  cite-method: biblatex
  #  #latex-output-dir: "../drafts"
  #  keep-tex: true
execute:
    echo: false
    message: false
    warning: false
    eval: true
params:
    windows: true
    reproduce: false
---

{{< pagebreak >}}

```{python}
#| tags: [parameters]

reproduce = "false"
```

# Introduction

Missing values are common in social science data. @king_et-al_2001_apsr estimate that political scientists lose about one-third of their data in their complete case regression analyses due to missing data. There are many reasons why missing values arise in our data. In surveys, these are referred to as item-nonresponse and pose threats to obtaining unbiased estimates for public opinion research when researchers utilize listwise deletion (`LWD`) in their regression analyses [@weisberg_2005_chicago]; mainly when the researcher can predict the cause of the missingness with an observed cause - which scholars refer to the data as "missing at random" [@king_et-al_2001_apsr].[^1] More generally, missing values come about in other types of data due to the unit's (e.g., country, respondent, politician) attempt to obfuscate information, data collector error, or researcher error. When common and not from a stochastic data generating process (DGP), missing data pose threats to causal inference through omitted variable bias where the missing value in the variables included in a statistical model can be predicted by another variable in your model.

[^1]: Which is a common occurrence, more so than missingness caused entirely by random chance - which, if one meets this condition, gives unbiased estimates with LWD.

As this poses a serious threat to our ability to make conclusions about causal processes, several political scientists generate tools and learn from other fields to deal with these challenges due to the severity of the consequences for inference. There are four primary approaches that political scientists use in dealing with missing data: (1) listwise deletion or complete case analysis (LWD), (2) simple mean or median based imputation, (3) hot deck or regression-based imputation, or (4) multiple imputation (MI). Many of the most popular approaches to imputation carry assumptions that scholars have to satisfy. This manuscript aims to provide a primer to political scientists on the use of machine learning models in the Multiple Imputation with Chained Equations (`MICE`) framework - an extension of the familiar MI approach.

Machine learning with `MICE` is not a new approach. While a JSTOR search of Political Science journals reports a handful of articles that discuss this particular approach, it is certainly not widely used by political scientists nor has it received much accessible discussion of how it works under the hood.[^2]

[^2]: Though, see @marbach_2022_pa.

As the present article discusses, `MICE` allows the researcher to choose various models to aid the imputation process. Selecting a model with its underlying assumptions provides many benefits for researchers in choosing a procedure that is *most* appropriate for their particular circumstances. Choosing models that allow for flexibility and models that do not have a number of restrictive assumptions benefit those unsure about the exact data-generating process of the missingness. The specific emphasis in this paper on using random forest models in `MICE` (`RF-MICE`) results from the strong preference of those who use machine learning models for predictive regressions to use random forest models due to their flexibility as a result of their fully non-parametric nature and for their performance under many different, and complex, circumstances [see @montgomery_olivella_2018_ajps; @james_et-al_2013_springer]. Fields like the biomedical sciences treat missing values as out-of-sample predictions that random forest models predict; they see the purpose of imputation as aligned with a task that random forest models are optimal. As other fields use this `RF-MICE` procedure, there are implementations in `STATA`, `R`, and `Python`. All of which are relatively easy in terms of knowledge to code in either of these languages [@buuren_goothuis-oudshoorn_2011_jss]. To be clear, this manuscript argues that we should not only consider using the `MICE` framework for imputation but also should consider using Random Forest models.

This manuscript compares the utility of random forest models in the `MICE` procedure for imputation to other common imputation tools used in political science. In doing this, the manuscript encourages political scientists to consider this procedure when faced with conditions where missing data are present, and the other common tools seem unsuitable. It is not to paint these models as superior in absolute terms to other procedures for imputation. To do that is a waste of time given the variety of circumstances where some methodological tools are suitable in some circumstances and not for others. This manuscript provides a primer encouraging political scientists to consider this tool when faced with missing data and to give them enough background so they may be comfortable using it. Furthermore, the manuscript agrees with the common recommendation that practitioners should recognize the value of reducing one's dependence on a single procedure and contends that one must consider using multiple procedures to reduce the dependency of one's results on any given procedure. This article provides code snippets that readers can use to implement all of these procedures in R.

The next section reviews common approaches handling missing data that political scientists currently use. The next section describes machine learning and random forest models and links this to my claim of their utility for predicting missing data when used in the `MICE` procedure. I then move into applied examples where I examine the performance of these random forest models to other common approaches to dealing with missing data on simulated data. I then discuss recommendations for when one should use the reigning popular techniques or the random forest application in the `MICE` procedure.

```{=html}
<!-- 
    END OF INTRODUCTION SECTION
-->
```
# Types of missing data, imputation, and MI in Political Science

## MCAR, MAR, and MNAR

Missing data arise in different forms. Researchers describe missing data in three ways - often using somewhat unintuitive acronyms. The first form missing data takes is Missing Completely At Random (`MCAR`). This means that the data generating process for the missingness is random - there are no observed or unobserved causes of missingness. The second form missing data takes is Missing At Random (`MAR`). In `MAR`, these data are missing due to some observed cause. However, they are "Missing at Random" once you account for that observed cause of missingness. Some argue that `MAR` is much more common given the state of how large most contemporary social science data sets are [@schunk_2008_atsa]. The third form that missing data take is Missing Not At Random (`MNAR`)[^3]. `MNAR` happens when observed and unobserved causes explain the missingness. What distinguishes `MNAR` from `MAR` is that the researcher does not have a clear path forward to handle the cause of missingness. This occurs either because the variable where there is missingness is, itself, a cause of the missingness, or data explaining the cause of missingness is unobserved by the researcher. Since missing data take different forms, researchers use a few different approaches to deal with these challenges. I summarize the types of missing data problems there are and their potential ramifications in @tbl-summary-missing-processes and the common tools used to solve these problems in @tbl-summary-missing-solutions.

[^3]: Sometimes called Non-ignorable (`NI`).

## Dealing with missingness

At the time of writing, @king_et-al_2001_apsr estimated that 94% of political scientists use `LWD` to deal with missing data. In short, `LWD` does not seek to impute missing values. Instead, if the dependent variable or any of the covariates in a regression model for a given observation are missing, the researcher does not include that observation in the analysis. Traditionally, scholars argue that `LWD` performs best (in terms of reducing the resulting bias in the researcher's subsequent regression models) when the data are `MCAR`.

If the data are `MAR` or `MNAR`, deleting observations with missing data introduces bias in one's regression estimates through a failure to account for correlation between the independent variable(s) and the error [@king_et-al_2001_apsr; @weisberg_2005_chicago; @schunk_2008_atsa; @azur_et-al_2011_ijmpr]. Furthermore, it has the potential to decrease statistical power. A meta-analysis of comparative and international political economy papers that use `LWD` demonstrates that political scientists have much, upwards of 50%, more Type I error - an incorrect rejection of the null hypothesis - than we would expect as a result of how we implement `LWD` [@lall_2016_pa]. To visualize this, we can draw a directed acyclic graph; @fig-bias.

```{mermaid}
%%| label: fig-bias
%%| fig-cap: MAR data as confound
%%| fig-width: 3
%%{init: {'theme':'base', 'themeVariables':{'primaryColor':'#ffffff', 'primaryBorderColor':'#000000'}}}%%

graph LR;
    a[X<sub>m</sub>] --> b[Y]
    c[X<sub>c</sub>] --> a
    d[M] --> a
```

Others push against this claim and instead argue that `LWD` does not inherently generate bias for non-`MCAR` data but that researchers neglect to control for the cause of `MAR` or `MNAR` [@arel-bundock_pelc_2018_pa]. This is still dependent on the researcher's grasp of theory and ability to identify the DGP leading to the missingness. Though the onus is on the researcher to do this, it is often a relatively high standard given the complexity to which social phenomena relate and the tendency for our datasets to be highly-dimensional. Furthermore, this also increases the number of parameters one must include in their statistical models - which runs the risk of increasing collinearity [@schrodt_2014_jopr].

Like `LWD`, simple imputation techniques like mean-and-median-based imputation do not reduce the chances of biased regression estimates. These approaches, called interpolation and extrapolation, are common for panel data and cross-sectional time series data. If you have missing data for an observation in one panel, you can take the same observations' responses in a previous panel and a latter panel. You then take the mean or the median of that particular observation for that variable. Other approaches seek to reduce this `MAR`-based bias through conditioning on other variables.

Hot Deck approaches to imputing missingness are regression-based in that they define the dependent variable as the one the researcher is attempting to impute and use variables thought to predict the cause of missingness in `MAR` contexts [@schunk_2008_atsa]. In this approach, you often use a few variables to condition on. In many cases, the precise mechanism generating missingness is often tricky to triangulate. As a result, if you fail to provide the correct model specification when the data are `MAR`, you often end up with biased regression estimates.

`MI` seeks to solve this issue by using the entire dataset for imputing missing values [@rubin_1996_jasa]. This approach uses the other variables in the dataset to generate a joint posterior distribution of all possible missing values for that particular observation. Many assume that most social science data sets are sufficiently large enough to condition on the mechanism generating `MAR` [@schunk_2008_atsa]. Unlike the other approaches, `MI` also generates uncertainty around the imputed values - via its construction of the joint posterior distribution [@rubin_1996_jasa] - which enables the researcher to be more transparent about the validity of those imputed values and to include that uncertainty in the researcher's subsequent statistical analyses [@king_et-al_2001_apsr; @von-hippel_2015_semmj]. A prevalent implementation of `MI` in political science is the `AMELIA II` software [@amelia; @honaker_et-al_2011_jss; @lall_2016_pa]. This useful tool provides a computationally fast and simple process for imputation. Compared to the other approaches to missing data, `AMELIA II` performs quite well [@honaker_et-al_2011_jss; @kropko_et-al_2014_pa]. `MI`, however, often requires a set of distributional assumptions for the joint distribution - often the multivariate normal [@honaker_et-al_2011_jss]. Another challenge with this tool is that it runs up with the curse of dimensionality -- if you are asking for more information by using more variables than you have observations, many non-regularized models will provide inaccurate estimates.

There is a variant to `MI` that seeks more computational efficiency and loosens some of the distributional assumptions required. This variant is called Multiple Imputation through Chained Equations (`MICE`). `MICE` performs quite well for large imputation tasks. `MI` struggles to impute values when there is missingness in the other variables of the dataset [@kropko_et-al_2014_pa] - which is quite common. Though not reliant on a multivariate normal distribution, Conditional `MI` still relies on general linear models (GLM) in calculating the values. `MICE` tries to get around this limitation in a few steps, as described by @azur_et-al_2011_ijmpr. @fig-process provides a visual representation of the procedure for a form of `MICE` used in this manuscript. I include more details about random forest models in the following subsection.

```{mermaid}
%%| label: fig-process
%%| fig-cap: Steps of RF-MICE procedure
%%| fig-width: 5
%%{init: {'theme':'base', 'themeVariables':{'primaryColor':'#ffffff', 'primaryBorderColor':'#000000'}}}%%
graph TD;
A[Missing Data] --> B[Simple Imputation]
B --> C[Select Variable and remove imputed values]
C --> D[Random forest model to impute predicted values]
D --> |Repeat for each variable| C
```

First, `MICE` performs a simple imputation, or interpolation, for every missing value in the entire dataset. These are the placeholder values. The second step in the general `MICE` paradigm involves identifying one variable to impute. Once complete, it then removes those placeholder values. The third step then involves regressing the observed values of the variable on the other variables in the model and replacing the predicted values generated from the regression model for the missing values. The fourth step is to repeat steps two and three for each variable in the data set with missing values - this constitutes a single iteration. As a fifth step, you perform between five and ten iterations[^4].

[^4]: Though, the exact number of recommended iterations used in `MICE` are still up for debate [see @buuren_goothuis-oudshoorn_2011_jss; @azur_et-al_2011_ijmpr]. The recommendation is that you elect to go with more iterations if not constrained by computational limitations.

The advantage of this chained equation procedure is to estimate each variable as an outcome with its own regression model that is most appropriate for it. The regression models that one may use in `MICE` are as numerous as those a researcher may choose from when engaged in statistical analysis. This means that the assumptions and the performance of the model one uses for the imputation are the same as in standard statistical analyses. Though it decreases some of the requirements for modeling the `MAR` process, it is not entirely atheoretical. We can, however, reduce the dependence that the imputed data have on a researcher's ability to theorize about the `MAR` process by selecting models that are accustomed to dealing with a large number of parameters without increasing inefficiency.

One valuable model for allowing one to include a large number of parameters without losses to efficiency, is a form of ensemble machine learning model called Random Forests. As the following discussion highlights, random forest models are optimal for engaging in predictive tasks [see @montgomery_olivella_2018_ajps], which appears appropriate for the task of predicting missing values. These models have the additional benefit of not requiring a multivariate normal distribution, not requiring one to specify a potentially incorrect model of which variables are included in the `MAR` process, nor is it in the form of a `GLM`. That is, these models reduce some of the dependence of an imputation task on the researcher's beliefs about the source of the missingness. As researchers apply these random forest models within the `MICE` framework, they also benefit from the advantages that `MI` provide over the hot deck framework. Furthermore, machine learning models like ensemble procedures apply regularization to deal with highly dimensional datasets. These three features suggest that this procedure offers much more flexibility to the researcher.

| Type | Cause                                                                                                                                               | Problem                                                                                     |
|-----------------|------------------------------|-------------------------------|
| MCAR | Missing data patterns are stochastic                                                                                                                | Does not cause bias in estimates                                                            |
| MAR  | Missing data patterns are not stochastic; however, once accounting for observed causes of missingness, any remaining missing data are as-if random. | Generates a type of omitted variable bias without accounting for it.                        |
| MNAR | Missing data patterns are not stochastic; caused by unobserved sources.                                                                             | Generates a type of omitted variable bias; hard to correct for as the source is unobserved. |

: Types of missing data processes, problems, and solutions {#tbl-summary-missing-processes}

| Procedure | Assumptions                                                                                                                                                                                                                                                                                                                                 | What it does                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Fixes                                       |
|--------------------|----------------------|-----------------------------------|--------|
| LWD       | Sources of missingness are random.                                                                                                                                                                                                                                                                                                          | Removes rows that have a missing value for *any* variable in the statistical model                                                                                                                                                                                                                                                                                                                                                                                                                                                             | MCAR                                        |
| Simple    | Sources of missingness may be explained by some type of autocorrelation (spatial, temporal, non-independence of observations).                                                                                                                                                                                                              | Takes the average or median value of observations that occur either before (if temporal data), proximate to (if spatial data), or similar to (if in the same sample), and fills that value into all rows with missingness for that variable.                                                                                                                                                                                                                                                                                                   | Data with autocorrelation or are not i.i.d. |
| Hotdeck   | Regression-based. So assumptions made are dependent on the particular regression you choose for this procedure.                                                                                                                                                                                                                             | Fits a regression model (specified by the researcher) where the researcher regresses the variable with missing values on other columns in the dataset. Fills empty rows with their predicted values from the regression model. Assumes that the regression model is properly specified.                                                                                                                                                                                                                                                        | MAR                                         |
| MI        | Assumes a normally distributed joint posterior distribution. Assumes that a GLM is appropriate for describing the non-stochastic sources of missingness.                                                                                                                                                                                    | Fits a Bayesian Linear Regression. Iteratively regresses each variable containing missing values onto all other variables in the dataset. Completes this process multiple times to account for uncertainty in the particular construction of the posterior distribution.                                                                                                                                                                                                                                                                       | MAR                                         |
| RFMICE    | MICE, in general, is constrained by the assumptions of the particular estimator one chooses. RFMICE, however, is designed to loosen a number of assumptions. Still in the Frequentist framework so it does not produce posterior distributions to reflect uncertainty. Uncertainty is reflected by variation within and between iterations. | First performs simple imputation on each variable that has missingness. Then fits a random forest model where it attempts to predict each variable containing missingness using all other variables present in the dataset. Within each iteration, Random Forest models perform their own iterative procedures to maximize predictive capacity using cross-validation. It performs this for each variable and produces some user specified number of datasets which are the result of those maximally predicted values for each missing value. | MAR                                         |

: Common solutions for missing data {#tbl-summary-missing-solutions}

```{=html}
<!-- 
    END OF LIT REVIEW
-->
```
# The utility of random forest models for imputing political science data

Random forest models are concerned with calculating a fixed out-of-sample prediction under the supervised machine learning framework. They are popular among data scientists and researchers primarily interested in providing predictions instead of engaging in causal inference. In non-imputation applications of supervised machine learning models - a broader category of machine learning models that includes random forests, these models take partial, observed information about an outcome and estimate the relationship between the units with observed outcomes and several other observed features for those units. Then for the units without an observed outcome, we generalize that relationship to predict an outcome for them.

To get an intuition of the fundamental goal of a supervised machine learning model, I will provide an analogy. If we are looking to sort beans into a good or bad pile before we toss them into a pot, we often want to collect information about them. Features like color, size, and plumpness can all be good indicators of whether a bean will taste good or bad. Say we have over 5,000 beans, and we are a chef at a restaurant approaching the dinner hour, and we do not have time to sort all these beans. To save time, we look at these features like color, size, and plumpness and make two piles - good or bad for a subset of our 5,000 beans; say, in this instance, about 25%. We then get one of our employees to sort the rest of the beans for us. This employee may know less than us about what features matter more and how to identify a bean that will taste good or bad. Nevertheless, they can look at the two piles we have already made and try to pick up on patterns that make good and bad beans different from one another. With this information, the employee can "learn" these patterns so that even without the same knowledge as the chef, they can still make predictions about whether a bean will taste good or bad.

We have some units with recorded observations for a particular variable for imputation. Leveraging this, we can treat these documented observations as information so that we can "train" our computer to find a relationship between the observed information in that variable and information from other variables for that unit. We can then generalize this relationship to predict what that value would be for a missing unit. This seems like a reasonable approach to thinking about imputing missing values. We are not making naive imputations by taking the mean value. As we are using an expanded form of `MI`, we can also use all variables in the dataset to clarify that pattern. As a `MICE` procedure, we can specify a machine learning model, and we do this iteratively so that if we have more than one variable that we want to impute, we are not limited to accurate imputed values only for the units that are complete except for that particular variable to be imputed. I will elaborate more on how this works in a few paragraphs. Before we go there, however, I want to take the time to provide a few more details about how random forest models work, as they do not represent the whole of supervised machine learning.

Practitioners refer to random forests as tree-based ensemble models. As a supervised machine learning model, they start with the basic intuition described above; but in the process of "learning" or "training," these models follow a few distinct steps. Decision tree models split regions of the predictive space. For example, say we want to predict vote choice by one's partisanship. We would split the predictive space into regions. These regions could be different degrees of partisanship, like strong Republicans and weak Republicans. When we split this predictive space into regions, we essentially are subsetting our dataset of partisans into these different areas of the predictive space and trying to optimize a model to provide the best predictions in each region in our predictive space. That is, we try to find a model that maximizes the predictive performance of vote choice for strong Republicans, weak Republicans, independents, and so on. We examine performance by comparing how well we are predicting the unobserved outcomes relative to the observed outcomes within those predictive regions. We can "bag" our trees. When we "bag" the decision trees, we bootstrap the training sample and build a tree for each bootstrapped sample and average across them. Doing this allows for a decrease in the variance of the predictions coming from the model, which helps with reducing the chances of generating a trained model that will fail to adequately generalize to our data set not included in the training step.

As this is a primer for the applied researcher, I note that this discussion simplifies decision trees and random forest models. For those interested in more details about these concepts, @james_et-al_2013_springer provide a helpful discussion of these concepts. The main point is that random forest models specialize in generating predictions by optimizing at the *value* and *not* the *variable* level. This characteristic suggests that these models have the potential to be powerful tools for many different applications.

We can discuss their applicability to imputation in the `MICE` framework with a foundation in how random forest models work. As random forest models specialize in generating fixed out-of-sample predictions, these models have a joint goal with multiple imputation in that it should not be evaluated on the model's correctness but on the model's ability to predict a fixed (true) value [@rubin_1996_jasa]. Here, one might think of missing data as the out-of-sample predictions intended to be estimated. With random forest models, you train the model on a training data set (often randomly generated through cross-validation), a randomly selected portion of your data that you train the model on, and then fit the model on the testing set, the remaining data not used for the training stage of your model [@hastie_et-al_2009_springer].

OLS models often perform relatively poorly on making out-of-sample predictions as they are `BLUE`, assuming the data on hand are relatively representative of the population. As we have `MAR` data, this assumption likely fails, and any out-of-sample predictions are likely to be biased due to overfitting. Further, OLS may also generate out-of-bounds predictions for non-continuous data [@long_1997_sage; @gelman_et-al_2021_cup] which is particularly troublesome in settings of prediction.

Other models provide within-bounds predictions, such as logistic models; however, as generalized linear models, they still assume a linear functional form and often produce biased interpretations of the likelihood function when presented with unobserved systematic processes [@mood_2010_esr]. Though OLS and Logistic regression underlie a lot of machine learning as tools, many consider them to have limited applicability to complicated settings requiring prediction. Random forest models provide within-bounds predictions, and they are fully non-parametric [@hastie_et-al_2009_springer], meaning they do not assume a functional form and consequently a joint distribution. This means that we have more flexibility in terms of what variables we have in our datasets that we need to impute. As social scientists, we rarely have datasets that contain `DGP`s that fall neatly within the optimal realm for `GLM`s. This added flexibility by using `MICE` and random forests makes the researcher's job easier.

On other performance metrics, random forest models, as an ensemble method, provide much more accurate predictions than single-tree alternatives in machine learning, such as CART [@montgomery_olivella_2018_ajps]. As discussed above, rather than generating a single estimate from a single model, ensemble models, like random forests, calculate multiple models and learn from their performance; this is the purpose of using the bootstrapped samples.

In the context of using `MICE`, I argue that political scientists should *consider* using random forest models to make accurate predictions with fewer assumptions and be more lenient in terms of conditioning on the cause of `MAR` in the data set. Recall that within each `MICE` iteration, one performs a model predicting (imputing) a missing value based on the other variables in the dataset. Explicitly, this means you are running a model per variable with missing data.

Given that random forests are non-parametric, within each `MICE` iteration, the relationship between the variable to be imputed and those used to make the predictions can be non-linear and can take many different multivariate distributional forms. This is a significant advancement on traditional MI, which assumes a multivariate normal distribution. Additionally, this is an advancement on other `MICE` models that political scientists use, which may not inherently conceptualize missing data as these unobserved values to predict from a generalized relationship of the observed variable with the other variables in the dataset. These relationships are also not assumed to be linear. Furthermore, using `RF-MICE` has the advantage over hot-deck procedures for those unsure about a variable's precise DGP for MAR.

In the next section, I illustrate the use of random forest models for political scientists by demonstrating a simulated application of a random forest implementation of `MICE`. The following section also compares this implementation's performance to other common approaches to handling missing data in political science in terms of our ability to reduce unobserved bias in our data and in computational costs. <!--
    END OF KEY SECTION FOR ARGUMENT
-->

# Application of `MICE` with random forests

```{python}
#| label: setup-block
# import modules
    #* from env
from itertools import repeat
from timeit import default_timer
import numpy as np
import duckdb as db
import miceforest as mf
import polars as pl
import plotly.figure_factory as ff
import plotly.io as pio
    #* user defined
from helper import simulate, ampute, transformation, impute, create_iter_table, mean_benchmarks
# Seed
np.random.seed(90210)
# Database
engine=db.connect("../data/sim_original.db")
engine2=db.connect("../data/sim_amputed.db")
# Loop parameters
n: int=100
datasets: int=1000
m: int=10
# Default template
    #* define default template
pio.templates.default = "plotly_white"
    #* define line colors for distplots
dist_color = []
for i in range(datasets):
    i = "#D3D3D3"
    dist_color.append(i)
```

Using the `numpy` library [@numpy], I simulate a population where $N=1000000$. The population has 5 variables with the `DGP`s presented in @eq-sim-dgp.

$$
\begin{split}
a_i = Gamma(2,2) \\
b_i = Binomial(1,0.6) \\
x_i = 0.2 \times a_i + 0.5 \times b_i + Normal(0,1) \\
z_i = 0.9 \times a_i \times b_i + Normal(0,1) \\
y_i = 0.6 \times x_i + 0.9 \times z_i + Normal(0,1)
\end{split}
$$ {#eq-sim-dgp}

I then use the `polars` library [@polars] to generate 1000 random samples from the population with $n = 100$ for each sample. To introduce missingness into my data, I use the `miceRanger` library [@miceRanger] to "ampute" 40% of the data for each of these samples with a `MAR` process. As one of the advantages of the `RF-MICE` procedure is to impute data generated from more complicated `MAR` processes, I ampute the data by constructing a logistic regression for each variable. Where the predicted value equals one, the corresponding observation is counted as missing [@miceRanger]. For the reader, I use the Plotly library [@plotly] to produce @fig-sim-dist; which presents the distributions of data for the `X`, `Z`, and `Y` variables for my samples *before* amputation as well as the distribution of data for the `X`, `Z`, and `Y` variables for my samples *after* amputation. I would like to note, there are other ways I can perform the amputation of the data. The particular DGP of the missing data here is relatively simple relative to what we may have in real-world situations. However, given that `RF-MICE` is expected to perform better in situations with more complicated missing data patterns, the results of the simulated analyses may be an understatement of the differences in the reduction of bias between these different tools.

```{python}
#| label: simulated-generation-block

# Execute all of the following if params.reproduce was set to true
if reproduce == "true":
    # Create 1000 datasets and store it in a list
    pop_data_frame, sim = simulate(num_obs = n, samples = datasets)

    # Take the original simulated datasets and store them in the database
    for i in range(datasets):
        data_frame = sim[i]
        create_iter_table(
            data_frame = data_frame,
            name_append = "original",
            i = i,
            engine = engine
        )
    # take the simulated dataframes and produce three dataframes for each x, y, and z variable
    x_list, z_list, y_list = transformation(
        data_frames = sim,
        datasets = datasets
    )

    # convert these dataframes to pandas to work nicely with plotly
    x_list_pandas = x_list.to_pandas()
    z_list_pandas = z_list.to_pandas()
    y_list_pandas = y_list.to_pandas()

    # Create density plots of variables across datasets
        #* For x variable
    fig_x = ff.create_distplot(
        [x_list_pandas[c] for c in x_list.columns], # grab the columns in the dataframes
        x_list.columns,
        colors=dist_color, # grab the names of the dataframe columns
        show_hist = False, # do not show the histogram
        show_rug = False # do not show the rugplot
    )
        #* For z variable
    fig_z = ff.create_distplot(
        [z_list_pandas[c] for c in z_list.columns], # grab the columns in the dataframes
        z_list.columns,
        colors=dist_color, # grab the names of the dataframe columns
        show_hist = False, # do not show the histogram
        show_rug = False # do not show the rugplot
    )
        #* For y variable
    fig_y = ff.create_distplot(
        [y_list_pandas[c] for c in y_list.columns], # grab the columns in the dataframes
        y_list.columns,
        colors=dist_color, # grab the names of the dataframe columns
        show_hist = False, # do not show the histogram
        show_rug = False # do not show the rugplot
    ) 
        #* remove the legends for each of these plots
    fig_x.update_layout(showlegend=False)
    fig_z.update_layout(showlegend=False)
    fig_y.update_layout(showlegend=False)
        #* store an image of each of these plots
    fig_x.write_image("../assets/pop_x_dist.png")
    fig_z.write_image("../assets/pop_z_dist.png")
    fig_y.write_image("../assets/pop_y_dist.png")
```

```{python}
#| label: amputation-block

if reproduce == "true":
    # Take each of the dataframes in the sim object, ampute them, and return a list object containing all of the dataframes
    amputed = list(map(lambda x: ampute(x, perc = 0.4, random_state = 90210), sim))
    # Store results in database
    for i in range(datasets):
        data_frame = amputed[i]
        create_iter_table(
            data_frame = data_frame,
            name_append = "amputed",
            engine = engine2,
            i = i
        )
    # take the amputed dataframes and produce three dataframes for each x, y, and z variable
    x_list, z_list, y_list = transformation(
        data_frames = amputed,
        datasets = datasets
    )

    # take the three dataframes, drop missing values and convert to pandas
    x_list_pandas = x_list.drop_nulls().to_pandas()
    z_list_pandas = z_list.drop_nulls().to_pandas()
    y_list_pandas = y_list.drop_nulls().to_pandas()

    # create density plots of the variables across datasets
        #* for x variable
    fig_x = ff.create_distplot(
        [x_list_pandas[c] for c in x_list.columns], # grab the columns in the dataframe
        x_list.columns,
        colors=dist_color, # grab the column names from the dataframe
        show_hist = False, # do not show the histogram
        show_rug = False # do not show the rugplot
    )
        #* for z variable
    fig_z = ff.create_distplot(
        [z_list_pandas[c] for c in z_list.columns], # grab the columns in the dataframe
        z_list.columns,
        colors=dist_color, # grab the column names from the dataframe
        show_hist = False, # do not show the histogram
        show_rug = False # do not show the rugplot,
    )
        #* for y variable
    fig_y = ff.create_distplot(
        [y_list_pandas[c] for c in y_list.columns], # grab the columns in the dataframe
        y_list.columns,
        colors=dist_color, # grab the column names from the dataframe
        show_hist = False, # do not show the histogram
        show_rug = False # do not show the rugplot
    )
        #* remove the legends for these plots
    fig_x.update_layout(showlegend=False)
    fig_z.update_layout(showlegend=False)
    fig_y.update_layout(showlegend=False)
        #* store the plots as images
    fig_x.write_image("../assets/amputed_x_dist.png")
    fig_z.write_image("../assets/amputed_z_dist.png")
    fig_y.write_image("../assets/amputed_y_dist.png")
```

```{python}
#| label: close-db-connections
engine.close() # close engine connection
engine2.close() # close engine2 connection
```

::: {#fig-sim-dist layout-ncol="2"}
![X](../assets/pop_x_dist.png)

![X - amputed](../assets/amputed_x_dist.png)

![Z](../assets/pop_z_dist.png)

![Z - amputed](../assets/amputed_z_dist.png)

![Y](../assets/pop_y_dist.png)

![Y - amputed](../assets/amputed_y_dist.png)

Distributions of simulated data
:::

With these amputed datasets, I then apply some of the procedures I have discussed to impute these values. Interpolation is a quite simple procedure where I can fill in missing values by using the mean value of that particular variable for the non-missing observations. I perform this interpolation with the `mice` package [@mice] and iterate over it to provide 10 datasets. I also use the `AMELIA II` package [@honaker_et-al_2011_jss] to perform standard `MI` and also store 10 datasets from the iterations. I use a standard Bayesian linear model in the `MICE` framework with the `mice` package [@mice]. Bayesian linear models with uniform distributions or a weak prior distribution are similar to the familiar Ordinary Least Squares [@gelman_et-al_2021_cup]. As discussed before, the final procedure I use is a random forest in the `MICE` framework. I perform the `RF-MICE` procedure using the `mice` package [@mice] and an alternative package `miceRanger` [@miceRanger]. I can use the codeblock below to perform the imputation with each of the tools I use on any given dataset I would use in a realistic research setting.

```{.r}
#| label: imputation-code-block-example
#| echo: true
#| eval: false

# Install libraries
install.packages(
    c(
        "AMELIA"
        , "mice"
        , "miceRanger"
    )
)
# Load libraries
library(Amelia) # for MI
library(mice) # for many MICE and interpolation procedures
library(miceRanger) # for RF-Mice procedure
# Dataset

df

# Listwise Deletion
dfImputed <- df[complete.cases(df), ] # exclude rows that have missing values in any column

# Interpolation
dfImputed <- mice(
    df # dataframe
    , m = 10 # number of imputations
    , method = "mean" # mean interpolation
)

# Amelia
dfImputed <- amelia(
    df # dataframe
    , m = 10 # number of imputations
)

# Linear Bayesian MICE
dfImputed <- mice(
    df # dataframe
    , m = 10 # number of imputations
    , method = "linear" # Bayesian linear MICE
)

# RF-MICE with mice package
dfImputed <- mice(
    df # dataframe
    , m = 10 # number of imputations
    , method = "rf" # RF-MICE
)

# RF-MICE with miceRanger package
dfImputed <- miceRanger(
    df # dataframe
    , m = 10 # number of imputations
)
```

When producing the imputed datasets, I use the `tictoc` package to record the amount of time each procedure takes to complete the task on the 1000 samples as a measure of computational cost.[^5] As a number of factors may affect the absolute computational costs for these procedures (e.g., hardware, whether other applications or software are running, whether one uses parallelization, etcetera), I am primarily going to focus on the relative computational costs of each procedure.

[^5]: It is important to note that these benchmarks are based on a computer 32 GB of RAM, on a Intel i7-9700K processor at 3.60GHz with 8 cores using a NVIDIA GEForce RTX 2070 graphics card.

```{r}
#| label: simulation-r-setup-block

# Set seed  
set.seed(90210)
# Modularly load required functions
box::use(
    DBI = DBI[dbConnect, dbGetQuery, dbWriteTable, dbDisconnect],
    duckdb = duckdb[duckdb], #needs to be version 0.6.1 for both R and python
    mice = mice[ampute, pool, complete, mice],
    miceRanger = miceRanger[miceRanger],
    data.table = data.table[data.table],
    Amelia = Amelia[amelia],
    tictoc = tictoc[...],
    dplyr = dplyr[select, filter, bind_rows],
    tibble = tibble[tibble],
    ggplot2 = ggplot2[ggplot, geom_density, aes, theme_minimal, labs, geom_boxplot, ggsave],
    rstan = rstan[...]
)
require(data.table)# for miceRanger

# rstan options

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

# Load user defined functions
source("helper.R")
# Loop parameters
n <- 100 # size of each sample
datasets <- 1000 # size of number of samples

# Connect to databases
engine <- dbConnect(
    duckdb(),
    dbdir = "../data/sim_original.db",
    read_only = TRUE # read only mode
)
engine2 <- dbConnect(
    duckdb(),
    dbdir = "../data/sim_amputed.db",
    read_only = TRUE # read_only mode
)
engine3 <- dbConnect(
    duckdb(),
    dbdir = "../data/sim_imputed.db"
)
engine4 <- dbConnect(
    duckdb(),
    dbdir = "../data/benchmarks.db"
)
```

```{r}
#| label: r-imputations

if (params$reproduce == "true") {
    # Interpolation
    tic.clearlog() # clear the log for tic
    tic("Interpolate") # initialize time counter for Interpolate procedure
    interpolate <- impute(# perform interpolation imputation with mice package
        package="mice",
        meth="mean"
    )
    toc(# document how much time it took for this to complete
        log = TRUE,
        quiet = TRUE
    )

    # AMELIA
    tic("Amelia") # initialize time counter for amelia procedure
    amelia <- impute( # perform amelia imputation
        package="amelia",
        con = engine4
    )
    toc( # document how much time it took for this to complete
        log = TRUE,
        quiet = TRUE
    )

    # LMICE
    tic("LMICE") # initialize time counter for linear mice procedure
    lmice <- impute( # perform lmice imputation with mice package
        package="mice",
        meth="norm",
        con = engine4
    )
    toc( # document how much time it took for this to complete
        log = TRUE,
        quiet = TRUE
    )

    # RFMICE
    tic("RFMICE") # initialize time counter for rfmice procedure
    rfmice <- impute(  # perform rfmice imputation with mice package
        package="mice",
        meth="rf",
        con = engine4
    )
    toc( # document how much time it took for this to complete
        log = TRUE,
        quiet = TRUE
    )

    #RFRanger
    tic("RFRanger") # initialize time counter for rfmice procedure
    rfranger <- impute(  # perform rfmice imputation with miceRanger package
        package="miceRanger",
        con = engine4
    )
    toc( # document how much time it took for this to complete
        log = TRUE,
        quiet = TRUE
    )

    #Store timing info
    log <- tictoc::tic.log(format = TRUE) |> # take the logged timing info and store it as a data.frame object
        stringr::str_split( ": ", simplify = TRUE) |>
        data.frame()
    log$X2 <- as.double( # store it as an integer in the X2 column
        gsub("[secelapsed]","",as.character(log$X2)) # remove the secelapsed prefix
    ) 
    setnames(  # set the column names
        log, # of the log dataset
        c("procedure","seconds") # to procedure and seconds
    )
        #* put it in a database
    dbWriteTable(
        engine4, # connect to the engine4 database
        "benchmark", # name the table "benchmark"
        log, # store the log data.frame
        overwrite = TRUE # and if it already exists, overwrite it
    )

    # Store imputed data in database
    for (i in 1: datasets) {
        df <- interpolate[[i]] # take each dataframe from the interpolate list
        dbWriteTable( # and write each to a table
            engine3, # connect to the engine3 database
            paste0("mean_", as.character(i)), # the name of the table should be the combo of mean_i
            df, # should be the current df
            overwrite = TRUE # overwrite this if the table by the current name already exists
        )
    }
    for (i in 1: datasets) {
        df <- amelia[[i]] # take each dataframe from the amelia list
        dbWriteTable( # and write each to a table
            engine3, # connect to the engine3 database
            paste0("amelia_", as.character(i)), # the name of the table should be the combo of amelia_i
            df, # should be the current df
            overwrite = TRUE # overwrite this if the table by the current name already exists
        )
    }
    for (i in 1: datasets) {
        df <- lmice[[i]] # take each dataframe from the lmice list
        dbWriteTable( # and write each to a table
            engine3, # connect to the engine3 database
            paste0("lmice_", as.character(i)), # the name of the table should be the combo of lmice_i
            df, # should be the current df
            overwrite = TRUE # overwrite this if the table by the current name already exists
        )
    }
    for (i in 1: datasets) {
        df <- rfmice[[i]] # take each dataframe from the rfmice list
        dbWriteTable( # and write each to a table
            engine3, # connect to the engine3 database
            paste0("rfmice_", as.character(i)), # the name of the table should be the combo of rfmice_i 
            df, # should be the current df
            overwrite = TRUE # overwrite this if the table by the current name already exists
        )
    }
    for (i in 1: datasets) {
        df <- data.frame( # store each object from the rfranger list into a dataframe
            rfranger[[i]]
        ) 
        dbWriteTable( # and write each to a table
            engine3, # connect to the engine3 database
            paste0("rfranger_", as.character(i)), # the name of the table should be the combo of rfranger_i
            df, # should be the current df
            overwrite = TRUE # overwrite this if the table by the current name already exists
        )
    }
}
```

Each imputation procedure produced $m = 10$ datasets per simulated dataset, $s = 1000$. I have a total of $s \times m$ datasets. For each s dataset, I took the difference the values for each m dataset from the values in the complete dataset and took the average of these differences to give me a mean score of the discrepancy for each s dataset. Using the `GGPlot2` package [@ggplot2] I produce @fig-discrepancy-x, @fig-discrepancy-y, and @fig-discrepancy-z, which represents these mean discrepancy scores for the three variables that were originally imputed.

```{r}
#| label: calculating-average-discrepancies
        #** Mean
interpolate_diff <- discrepancy( # calculate the value-wise discrepancy
    procedure = "interpolate" # for the interpolate procedure
)
        #** Amelia
amelia_diff <- discrepancy( # calculate the value-wise discrepancy
    procedure = "amelia" # for the amelia procedure
)
        #** LMice
lmice_diff <- discrepancy( # calculate the value-wise discrepancy
    procedure = "lmice" # for  the lmice procedure
)
        #** RFMice
rfmice_diff <- discrepancy(# calculate the value-wise discrepancy
    procedure = "rfmice" # for the rfmice procedure
)
        #** RFRanger
rfranger_diff <- discrepancy(# calculate the value-wise discrepancy
    procedure = "rfranger" # for the rfranger procedure
)
```

```{r}
#| label: fig-discrepancy-x
#| eval: true
#| fig-cap: Discrepancies - X
ggplot() +
    geom_boxplot(
        aes(
            x = 'Mean',
            y = mean_x
        ),
        data = interpolate_diff
    ) +
    geom_boxplot(
        aes(
            x = 'AMELIA',
            y = mean_x
        ),
        data = amelia_diff
    ) + # boxplot for amelia procedure
    geom_boxplot(
        aes(
            x = 'Linear MICE',
            y = mean_x
        ),
        data = lmice_diff
    ) + # boxplot for lmice procedure
    geom_boxplot(
        aes(
            x = 'RF MICE',
            y = mean_x
        ),
        data = rfmice_diff
    ) + # boxplot for rfmice procedure with mice
    geom_boxplot(
        aes(
            x = "RF Ranger",
            y = mean_x
        ),
        data = rfranger_diff
    ) + # boxplot for rfmice procedure with miceRanger
    theme_minimal() + # add the minimal theme
    labs(
        x = 'X',
        y = 'Average difference from complete data'
    ) # adjust the labels to the axes
```

```{r}
#| label: fig-discrepancy-z
#| eval: true
#| fig-cap: Discrepancies - Z
ggplot() +
    geom_boxplot(
        aes(
            x = 'Mean',
            y = mean_z
        ),
        data = interpolate_diff
    ) + # boxplot for interpolation procedure
    geom_boxplot(
        aes(
            x = 'AMELIA',
            y = mean_z
        ),
        data = amelia_diff
    ) + # boxplot for amelia procedure
    geom_boxplot(
        aes(
            x = 'Linear MICE',
            y = mean_z
        ),
        data = lmice_diff
    ) + # boxplot for lmice procedure
    geom_boxplot(
        aes(
            x = 'RF MICE',
            y = mean_z
        ),
        data = rfmice_diff
    ) + # boxplot for rfmice procedure with mice
    geom_boxplot(
        aes(
            x = "RF Ranger",
            y = mean_z
        ),
        data = rfranger_diff
        ) + # boxplot for rfmice procedure with miceRanger
    theme_minimal() + # add the minimal theme
    labs(
        x = 'Z',
        y = 'Average difference from complete data'
    ) # adjust the labels to the axes
```

```{r}
#| label: fig-discrepancy-y
#| eval: true
#| fig-cap: Discrepancies - Z
ggplot() +
    geom_boxplot(
        aes(
            x = 'Mean',
            y = mean_y
        ),
        data = interpolate_diff
    ) + # boxplot for interpolation procedure
    geom_boxplot(
        aes(
            x = 'AMELIA',
            y = mean_y
        ),
        data = amelia_diff
    ) + # boxplot for amelia procedure
    geom_boxplot(
        aes(
            x = 'Linear MICE',
            y = mean_y
        ),
        data = lmice_diff
    ) + # boxplot for lmice procedure
    geom_boxplot(
        aes(
            x = 'RF MICE',
            y = mean_y
        ),
        data = rfmice_diff
    ) + # boxplot for rfmice procedure with mice
    geom_boxplot(
        aes(
            x = "RF Ranger",
            y = mean_y
        ),
        data = rfranger_diff
    ) + # boxplot for rfmice procedure with miceRanger
    theme_minimal() + # add the minimal theme
    labs(
        x = 'Y',
        y = 'Average difference from complete data'
    ) # adjust the labels to the axes
```

Overall, we see that the procedures yield somewhat small differences between the actual and estimated values. For Y, `AMELIA II` has a mean discrepancy of `r sprintf('%.3f', mean(amelia_diff$mean_y))`, the Linear `MICE` procedure has a mean discrepancy of `r sprintf('%.3f', mean(lmice_diff$mean_y))`, the interpolation procedure has a mean discrepancy of `r sprintf('%.3f', mean(interpolate_diff$mean_y))`, and `RF-MICE` with the `mice` package has a mean discrepancy of `r sprintf('%.3f', mean(rfmice_diff$mean_y))` and a mean discrepancy of `r sprintf('%.3f', mean(rfranger_diff$mean_y))`. For X, we see that the `RF-MICE` with the `mice` package has a mean discrepancy of `r sprintf('%.3f', mean(rfmice_diff$mean_x))` and `r sprintf('%.3f', mean(rfranger_diff$mean_x))` with the `miceRanger` package, interpolation has a mean discrepancy of `r sprintf('%.3f', mean(interpolate_diff$mean_x))`, Linear `MICE` has a mean discrepancy of `r sprintf('%.3f', mean(lmice_diff$mean_x))`, and `AMELIA II` has a mean discrepancy of `r sprintf('%.3f', mean(amelia_diff$mean_x))`. For Z, we also see that `RF-MICE` with the `mice` package has a mean discrepancy of `r sprintf('%.3f', mean(rfmice_diff$mean_z))` and `r sprintf('%.3f', mean(rfranger_diff$mean_z))` with the `miceRanger` package, interpolation has a mean discrepancy of `r sprintf('%.3f', mean(interpolate_diff$mean_z))`, Linear `MICE` has a mean discrepancy of `r sprintf('%.3f', mean(lmice_diff$mean_z))` and `AMELIA II` has a mean discrepancy of `r sprintf('%.3f', mean(amelia_diff$mean_z))`.

Though it is not a novel claim, I argue that in situations where we have missingness due to a `MAR` pattern, our regression models suffer from bias due to the systematic process generating that missingness. What I have argued so far is that we should consider using `RF-MICE` as it is, relative to other `MI` tools, a flexible tool that may be able to model a number of `MAR` processes which would help with reducing bias in our regression models.

To examine this claim, I take the amputed and imputed datasets (10) and use Rubin's rule [@rubin_1996_jasa] to pool across the regression models performed on each amputed and imputed sample. In total, I use the `rstan` package [@rstan] to fit \$`m` \times `s` \$ -- $10 \times 1000$ regressions where I pool across my average posterior draws from `m` to produce a single set of statistics for each `s`.[^6] I calculate the discrepancy of the pooled average posterior draws for `X` and `Z` -- the variables directly contributing to the DGP of `Y` -- from their parameters. I then present boxplots, produced by `GGPlot2` [@ggplot2], that demonstrates the differences in levels of bias between `LWD`, `AMELIA II`, Linear `MICE`, and `RF-MICE` across my samples. The specification for the regression is depicted in @eq-regression.

[^6]: As the model is not complicated and I am familiar with `Y`'s `DGP`, I fit the model using one chain with 1000 iterations.

$$
\begin{split}
    y_i ~ Normal(\mu_i, \sigma) \\
    \mu_i = \alpha + \beta_{1}(x_i - \bar{x}) + \beta_{2}(z_i - \bar{z}) \\
    \alpha = Uniform(-\infty, \infty) \\
    \beta_1 = Uniform(-\infty, \infty) \\
    \beta_2 = Uniform(-\infty, \infty) \\
    \sigma = Uniform(0, \infty) \\
\end{split}
$$ {#eq-regression}

For readers not familiar with this notation, @eq-regression essentially describes the familiar OLS regression. I fit a model where $y_i$ is explained by my likelihood function, $\mu_i$ and I assume that my parameters in my model can take any real number value. That is, I am not imposing a constraint with my defined priors about how large or small my estimates of $\beta_1$, $\beta_2$, and $\alpha$ can be. $\sigma$ depicts the variance, which I constrain to be positive.

In the codeblock below, I provide an example of how to take the output `dfImputation` from the example codeblock above and pool the regression results across the imputed data generated from each tool. I demonstrate how to do this with the `lm` function which fits a Ordinary Least Squares regression and with the `brms_multiple` function which fits a Bayesian equivalent to the Ordinary Least Squares regression when using the default uniform priors.

```{.r}
#| label: codeblock-example-of-regression
#| echo: true
#| eval: false

# Install packages
install.packages("brms") # for bayesian regression models
install.packages("miceadds") # for converting Amelia outputs

# Load libraries
library(brms)
library(miceadds)

# Listwise deletion
    #* With the lm command
regression <- lm(
    formula = Y ~ X + Z # Y is the dependent variable with X and Z as predictors
    , data = dfImputed # use the imputed data
)
    #* With brms
regression <- brms_multiple(
    formula = Y ~ X + Z # Y is the dependent variable with X and Z as predictors
    , data = dfImputed # use the imputed data
)

# Interpolation
    #* with the lm command
regression <- pool(
    with(
        data = dfImputed # use the imputed data
        , exp = lm(Y ~ X + Z) # Y is the dependent variable with X and Z as predictors
    )
) # pool across each of the imputed datasets
    #* With brms
regression <- brms_multiple(
    formula = Y ~ X + Z # Y is the dependent variable with X and Z as predictors
    , data = dfImputed # use the imputed data
)

# Amelia
    #* with the lm command
regression <- pool(
    with(
        data = datlist2mids(dfImputed$imputations) # use the imputed data
        , exp = lm(Y ~ X + Z) # Y is the dependent variable with X and Z as predictors
    )
) # pool across each of the imputed datasets
    #* With brms
regression <- brms_multiple(
    formula = Y ~ X + Z # Y is the dependent variable with X and Z as predictors
    , data = datlist2mids(dfImputed$imputations) # use the imputed data
)

# Linear bayesian mice
    #* with the lm command
regression <- pool(
    with(
        data = dfImputed # use the imputed data
        , exp = lm(Y ~ X + Z) # Y is the dependent variable with X and Z as predictors
    )
) # pool across each of the imputed datasets
    #* With brms
regression <- brms_multiple(
    formula = Y ~ X + Z # Y is the dependent variable with X and Z as predictors
    , data = dfImputed # use the imputed data
)

# RF-MICE with mice package
    #* with the lm command
regression <- pool(
    with(
        data = dfImputed # use the imputed data
        , exp = lm(Y ~ X + Z) # Y is the dependent variable with X and Z as predictors
    )
) # pool across each of the imputed datasets
    #* With brms
regression <- brms_multiple(
    formula = Y ~ X + Z # Y is the dependent variable with X and Z as predictors
    , data = dfImputed # use the imputed data
)

# RF-MICE with the miceRanger package
    #* with the lm command
regression <- pool(
    with(
        data = completeData(dfImputed) # use the imputed data
        , exp = lm(Y ~ X + Z) # Y is the dependent variable with X and Z as predictors
    )
) # pool across each of the imputed datasets
    #* With brms
regression <- brms_multiple(
    formula = Y ~ X + Z # Y is the dependent variable with X and Z as predictors
    , data = completeData(dfImputed) # use the imputed data
)
```

```{r}
#| label: regression-discrepancies

if (params$reproduce == "true") {
    #** compile the stan model 
    compiled <- stan_model(
        "ols.stan", # compile the model outlined in ols.stan file 
        model_name = "OLS" # name it OLS
    )
    # LWD
    lwd_reg_diff <- discrepancy( # calculate the discrepancy
        procedure = "amputed", # for the lwd procedure 
        model = TRUE # based on statistics-parameters
    )   
    # Mean
    interpolate_reg_diff <- discrepancy( # calculate the discrepancy
        procedure = "interpolate", # for the interpolate procedure
        model = TRUE # based on statistics-parameters
    )
    # Amelia
    amelia_reg_diff <- discrepancy( # calculate the discrepancy
        procedure = "amelia", #for the amelia procedure
        model = TRUE #based on statistics-parameters
    )
    # LMice
    lmice_reg_diff <- discrepancy(#calculate the discrepancy
        procedure = "lmice", #for the lmice procedure
        model = TRUE # based on statistics-parameters
    )
    # RFMice
    rfmice_reg_diff <- discrepancy(#calculate the discrepancy
        procedure = "rfmice", # for the rfmice procedure
        model = TRUE #based on statistics-parameters
    )
    # RFRanger
    rfranger_reg_diff <- discrepancy(#calculate the discrepancy
        procedure = "rfranger", # for the miceranger procedure
        model = TRUE #based on statistics parameters
    )

    dbWriteTable( #write to a table
        engine4, #in the engine4 database
        "lwd_reg_bias", #the table should be named this 
        lwd_reg_diff # the contents should be these data
    )
    dbWriteTable(#write to a table
        engine4, #in the engine4 database
        "interpolate_reg_bias", #the table should be named this
        interpolate_reg_diff# the contents should be these data
    )
    dbWriteTable(#write to a table
        engine4, # in the engine4 database
        "amelia_reg_bias", #the table should be named this
        amelia_reg_diff# the contents should be these data
    )
    dbWriteTable(#write to a table
        engine4, #in the engine4 database
        "lmice_reg_bias", # the table should be named this
        lmice_reg_diff # the contents should be these data
    )
    dbWriteTable(#write to a table
        engine4,#in the engine4 database
        "rfmice_reg_bias",#the table should be named this
        rfmice_reg_diff# the contents should be these data
    )
    dbWriteTable(#write to a table
        engine4,#in the engine4 database
        "rfranger_reg_bias",#the table should be named this
        rfranger_reg_diff# the contents should be these data
    )
} else {
    lwd_reg_diff <- dbGetQuery( #load from a table
        engine4,#in the engine4 database
        "SELECT * FROM lwd_reg_bias" # execute this query to get the data
    )
    interpolate_reg_diff <- dbGetQuery(#load from a table
        engine4, # in the engine4 database
        "SELECT * FROM interpolate_reg_bias" #execute this query to get the data
    )
    amelia_reg_diff <- dbGetQuery(#load from a table
        engine4, #in the engine4 database 
        "SELECT * FROM amelia_reg_bias" #execute this query to get the data
    )
    lmice_reg_diff <- dbGetQuery(#load from a table
        engine4, #in the engine4 database
        "SELECT * FROM lmice_reg_bias" #execute this query to get the data
    )
    rfmice_reg_diff <- dbGetQuery(#load from a table
        engine4, #in the engine4 database
        "SELECT * FROM rfmice_reg_bias" #execute this query to get the data
    )
    rfranger_reg_diff <- dbGetQuery(#load from a table
        engine4, #in the engine4 database
        "SELECT * FROM rfranger_reg_bias" #execute this query to get the data
    )
}

```

```{r}
#| label: fig-reg-discrepancy-x
#| eval: true
#| fig-cap: Average discrepancy scores - X
ggplot() + # make a plot object
    geom_boxplot(
        aes(
            x = 'LWD',
            y = mean_x
        ),
        data = lwd_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = 'Mean',
            y = mean_x
        ),
        data = interpolate_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = 'AMELIA',
            y = mean_x
        ),
        data = amelia_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = 'Linear MICE',
            y = mean_x
        ),
        data = lmice_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = 'RF MICE',
            y = mean_x
        ),
        data = rfmice_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = "RF Ranger",
            y = mean_x
        ),
        data = rfranger_reg_diff
    ) +
    theme_minimal() +
    labs(
        x = 'X',
        y = 'Average difference of point estimate from parameter'
    )
```

```{r}
#| label: fig-reg-discrepancy-z
#| eval: true
#| fig-cap: Average discrepancy scores - Z
ggplot() +
    geom_boxplot(
        aes(
            x = 'LWD',
            y = mean_z
        ),
        data = lwd_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = 'Mean',
            y = mean_z
        ),
        data = interpolate_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = 'AMELIA',
            y = mean_z
        ),
        data = amelia_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = 'Linear MICE',
            y = mean_z
        ),
        data = lmice_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = 'RF MICE',
            y = mean_z
        ),
        data = rfmice_reg_diff
    ) +
    geom_boxplot(
        aes(
            x = "RF Ranger",
            y = mean_z
        ),
        data = rfranger_reg_diff
    ) +
    theme_minimal() +
    labs(
        x = 'Z',
        y = 'Average difference from complete data'
    )
```

@fig-reg-discrepancy-x and @fig-reg-discrepancy-z present the distribution of differences between my average posterior draw and of my parameter value for the $\beta$ coefficient for `X` and for `Z` respectively. First looking at bias in the $\beta$ estimates for `X`, I see that the median discrepancy caused by `LWD` is quite small (close to zero). However, in looking at the spread, there are a number of models that produced biased estimates. It appears that the only two procedures that had a median of no bias was Linear `MICE` and `RF-MICE` with the `miceRanger` package [@miceRanger]. We see, that the spread of discrepancies is incredibly small when performing `RF-MICE` with the `miceRanger` package [@miceRanger]. `AMELIA II` [@amelia] performs quite well in terms of median levels of discrepancies, but still produces biased estimates on average, with some examples of extremely poor performance (in terms of bias). Turning to discrepancies between the $\beta$ statistics and parameters for `Z`, we see a very similar story. `LWD` produces a distribution of discrepancies that is quite concerning while `RF-MICE` as implemented through the `miceRanger` package [@miceRanger] does quite well at reducing bias caused by the `MAR` process. `AMELIA II` [@amelia] also does quite well in reducing bias, on average, however can vary quite significantly in its ability to do so.

```{r}
#| label: r-close
dbDisconnect(engine3, shutdown = TRUE)
dbDisconnect(engine4, shutdown = TRUE)
```

```{python}
#| echo: false

# Take the average time it took each procedure for each sample
interpolate_benchmark = mean_benchmarks(procedure="inter")
amelia_benchmark = mean_benchmarks(procedure="amelia")
lmice_benchmark = mean_benchmarks(procedure="lmice")
rfmice_benchmark = mean_benchmarks(procedure="rfmice")
rfranger_benchmark = mean_benchmarks(procedure="rfranger")
```

```{python}
#| output: asis

print(f"""Turning to the computational performance of each procedure, interpolation with the `mice` package [@mice] took an average of {interpolate_benchmark.select("mean_seconds")[0,0]:.2f} seconds; AMELIA II [@amelia] took an average of {amelia_benchmark.select("mean_seconds")[0,0]:.2f} seconds; linear `mice` took an average of {lmice_benchmark.select("mean_seconds")[0,0]:.2f} seconds; `RF-MICE` with the `mice` package [@mice] took an average of {rfmice_benchmark.select("mean_seconds")[0,0]:.2f} seconds; and `RF-MICE` with the `miceRanger` package [@miceRanger] took an average of {rfranger_benchmark.select("mean_seconds")[0,0]:.2f}.""")
```

```{=html}
<!--
# need this to display text for markdown output
from IPython.display import display, Markdown

# display the following text with a wonky python in-line code markdown work around
display(Markdown(f"""
Turning to the computational performance of each procedure, interpolation with the `mice` package [@mice] took an average of {interpolate_benchmark.select("mean_seconds")[0,0]} seconds; AMELIA II [@amelia] took an average of {amelia_benchmark.select("mean_seconds")[0,0]} seconds; linear `mice` took an average of {lmice_benchmark.select("mean_seconds")[0,0]} seconds; `RF-MICE` with the `mice` package [@mice] took an average of {rfmice_benchmark.select("mean_seconds")[0,0]} seconds; and `RF-MICE` with the `miceRanger` package [@rfRanger] took an average of {rfranger_benchmark.select("mean_seconds")[0,0]}.
"""
)) # This all should be pasted out into the manuscript
-->
```
```{=html}
<!--
    END OF METHODS SECTION
-->
```
# Conclusions

In theory, `RF-MICE` is a quite flexible tool that can operate in a number of circumstances to not only discover systematic processes leading to missingness in one's data, but to also use such information to recover the values. These expectations rely on the claim that Random Forest models are optimized for discovering patterns and to use that information to make out-of-sample predictions. With `RF-MICE` Random forests are coupled with `MICE` to produce significant improvements at retrieving the true values when a `MAR` process is present.

Using simulated data, I demonstrate two implementations of `RF-MICE` in `R` and compare it to implementations of more common procedures for dealing with `MAR` processes in political science. Overall, my simulated data and a number of measures of performance favor those theoretical expectations.

When comparing the distribution of imputed samples to the true samples, as if the `MAR` process was not present, both `RF-MICE` implementations do quite well in recovering the true values that were missing. When examining the discrepancy between what the imputed value is and the true value, the two `RF-MICE` implementations provide quite small discrepancies on average.

Measuring the performance of `RF-MICE` in terms of reducing bias in statistical estimation, I find that `RF-MICE`, when implemented with the `miceRanger` package [@miceRanger], stands out as a valuable tool for researchers to use to reduce bias generated with `MAR` processes.

While in theory, it is nice for the applied researcher to hear about a new and powerful tool or procedure, they face many constraints. When examining how much time each implementation of a procedure took, I observed that both implementations of `RF-MICE` were not significantly worse in time it took to complete the procedure relative to other multiple imputation procedures.

Altogether, the simulated data suggest that `RF-MICE` is a valuable procedure for the applied researcher's toolbox. When dealing with missing data that one suspects may not be the result of a `MCAR` process, one should consider the flexibility that `RF-MICE` provides. It does not require the researcher to specify the variables involved in the `MAR` process, but also does not introduce significant computational costs nor introduce so much complexity as a procedure that it is impossible to anticipate or diagnose problems the procedure may introduce.

It is important to remind the reader that all tools have their limitations and that their value are quite dependent on the context for which they are to be applied. It is useful, however, to include tools that vary in the assumptions they make [@neumayer_plumper_2017_cup]. The capabilities of `RF-MICE` are no different. While `RF-MICE` is quite flexible for addressing a number of `MAR` processes, it is important to note that it is not and should not be seen as a default or the sole tool for one to use when dealing with missing data. For example, simulations demonstrate that `RF-MICE` performs poorly when the missing data pattern arises from a moderating relationship [@marbach_2022_pa]. With the number of tools and their implementations being so available to the applied researcher, one should not shy away from using multiple implementations of these tools to ensure that one's substantive conclusions are not dependent on one tool or implementation. <!--
    END OF DOCUMENT
-->