---
title: |
  Seeing the leaves through the forest
subtitle: A primer on using random forest models for missing data problems
short-title: Seeing the leaves through the forest
code-repo: "Replication materials are stored at <https://github.com/DamonCharlesRoberts/mice-imputation-psci>"
author:
  #- name: Anonymized for review
  - name: Damon Charles Roberts
    email: damon.roberts-1@colorado.edu
    orcid: 0000-0002-4360-3675
    title: PhD Candidate
    affiliations:
      name: University of Colorado Boulder
      department: Political Science
      city: Boulder
      region: CO
      postal-code: 80309-0333
    attributes:
      corresponding: true
abstract: |
  Though missing data is pervasive in political science datasets, attempting to regain information from it remains a relatively uncommon step in data pre-processing. While there are many options out there, the benefits and drawbacks each provide can make it difficult to discern which to use. This note has two goals. First, to provide a review of the consequences of missing data and to provide a reference for common options used by political scientists. The second goal of the note is to advocate for the uptake of using random forest models in the Multiple Imputation with Chained Equations framework. In doing so, it lays out the intuition of these models and how that fits with the task of imputing missing data while also comparing the use of this implementation to other common approaches used in political science with simulated data that are representative of political science data.
thanks: |
  Word Count (including references): 5705. I would like to thank Madeline Mader, Alexandra Siegel, Andrew Q. Philips, and Jennifer Wolak for their advice and many conversations during the development of this project as well as Andy Baker for offering me the space to write the manuscript and for his feedback. I would also like to thank the discussants and panelists at MPSA for their useful feedback and encouragement.
published: "Working paper. Please do not distribute without author consent."
keywords:
  - Missing data
  - Multiple imputation
  - Machine learning
date: today
bibliography: "../assets/references.bib"
format:
  hikmah-pdf:
    # put the pdf in t he drafts directory
    latex-output-dir: "../out/"
    # Use biblatex-chicago
    cite-method: citeproc
    biblatex-chicago: true
    biblio-style: authordate
    biblatexoptions:
        - backend=biber
        - autolang=hyphen
        - isbn=false
        - uniquename=false
    filters: 
      - "_extensions/andrewheiss/wordcount/citeproc.lua"
      - "_extensions/andrewheiss/wordcount/wordcount.lua"
    geometry:
      - top=1in
      - bottom=1in
      - left=1in 
      - right=1in
      - heightrounded
    linestretch: 2
    fontsize: 12pt
    appendix-style: none
execute:
  echo: false
  message: false
  warning: false
  eval: true
params:
  reproduce: false
---

{{< pagebreak >}}

# Introduction

Missing values are common in social science data. @king_et-al_2001_apsr estimate that political scientists lose about one-third of their data in their complete case regression analyses due to missing data. This manuscript has two goals. First, to convince political scientists that the consequences of missing data should be avoided. Without the first goal, the second goal of the manuscript is likely to not be of use. The second goal of the manuscript is to provide an accessible introduction to the use of random forest models (`RF`) to fix a common pattern of missing data. 

While implementations of random forest models to solve missing data problems are not new, discussions of these tools are not readily accessible to most quantitative political scientists ^[Though see @marbach_2022_pa for an example in political science.] as these procedures are popular in fields such as Statistics and Bioinformatics.^[Using JSTOR's text mining tool on journals in political science reports that there were only about 40 published articles since 2010 that discuss "MICE" -- the common imputation framework that random forest models are implemented in -- and "imputation". [Link to search.](https://constellate.org/dataset/d2d027e8-4fb3-b2fd-51c8-93252aa50f15/?unigrams=election,%20voters)] In the following sections, I review the consequences of incomplete data in one's analysis, then I review existing techniques, then I describe how random forest models work and describe the intuition behind their implementation to solve familiar missing data problems. I finish the manuscript with simulations comparing different implementations of random forest models for imputation alongside common approaches to handling missing data in political science.
<!--
There are many reasons why missing values arise in our data. In surveys, these are referred to as item-nonresponse and pose threats to obtaining unbiased estimates for public opinion research when researchers utilize listwise deletion (`LWD`) in their regression analyses [@weisberg_2005_chicago]; mainly when the researcher can predict the cause of the missingness with an observed cause - which scholars refer to the data as "missing at random" [@king_et-al_2001_apsr].[^1] More generally, missing values come about in other types of data due to the unit's (e.g., country, respondent, politician) attempt to obfuscate information, data collector error, or researcher error. When common and not from a stochastic data generating process (DGP), missing data pose threats to causal inference through omitted variable bias where the missing value in the variables included in a statistical model can be predicted by another variable in your model.

[^1]: Which is a common occurrence, more so than missingness caused entirely by random chance - which, if one meets this condition, gives unbiased estimates with LWD.

As this poses a serious threat to our ability to make conclusions about causal processes, several political scientists generate tools and learn from other fields to deal with these challenges due to the severity of the consequences for inference. There are four primary approaches that political scientists use in dealing with missing data: (1) listwise deletion or complete case analysis (LWD), (2) simple mean or median based imputation, (3) single regression-based imputation, or (4) multiple imputation (MI). Many of the most popular approaches to imputation carry assumptions that scholars have to satisfy. This manuscript aims to provide a primer to political scientists on the use of machine learning models in the Multiple Imputation with Chained Equations (`MICE`) framework - an extension of the familiar MI approach.

Machine learning with `MICE` is not a new approach. While a JSTOR search on their text mining tool of Political Science journals reports that there were only about 40 articles since 2010 in Political science that discuss "MICE" and "imputation" despite the well-known problems that `LWD` produce.^[ [Link to search](https://constellate.org/dataset/d2d027e8-4fb3-b2fd-51c8-93252aa50f15/?unigrams=election,%20voters)] Fewer of these articles provide accessible discussions of how this procedure works.[^2]

[^2]: Though, see @marbach_2022_pa.

As the present article discusses, `MICE` allows the researcher to choose various models to aid the imputation process. Selecting a model with its underlying assumptions provides many benefits for researchers in choosing a procedure that is *most* appropriate for their particular circumstances. Choosing models that allow for flexibility and models that do not have a number of restrictive assumptions benefit those unsure about the exact data-generating process of the missingness. The specific emphasis in this paper on using random forest models in `MICE` (`RF-MICE`) results from the strong preference of those who use machine learning models for predictive regressions to use random forest models due to their flexibility as a result of their fully non-parametric nature and for their performance under many different, and complex, circumstances [see @montgomery_olivella_2018_ajps; @james_et-al_2013_springer]. Fields like the biomedical sciences treat missing values as out-of-sample predictions that random forest models predict; they see the purpose of imputation as aligned with a task that random forest models are optimal. As other fields use this `RF-MICE` procedure, there are implementations in `STATA`, `R`, and `Python`. All of which are relatively easy in terms of knowledge to code in either of these languages [@buuren_goothuis-oudshoorn_2011_jss]. To be clear, this manuscript argues that we should not only consider using the `MICE` framework for imputation but also should consider using Random Forest models.

This manuscript compares the utility of random forest models in the `MICE` procedure for imputation to other common imputation tools used in political science. In doing this, the manuscript encourages political scientists to consider this procedure when faced with conditions where missing data are present, and the other common tools seem unsuitable. It is not to paint these models as superior in absolute terms to other procedures for imputation. To do that is a waste of time given the variety of circumstances where some methodological tools are suitable in some circumstances and not for others. This manuscript provides a primer encouraging political scientists to consider this tool when faced with missing data and to give them enough background so they may be comfortable using it. Furthermore, the manuscript agrees with the common recommendation that practitioners should recognize the value of reducing one's dependence on a single procedure and contends that one must consider using multiple procedures to reduce the dependency of one's results on any given procedure. This article provides code snippets that readers can use to implement all of these procedures in R.

The next section reviews common approaches handling missing data that political scientists currently use. The next section describes machine learning and random forest models and links this to my claim of their utility for predicting missing data when used in the `MICE` procedure. I then move into applied examples where I examine the performance of these random forest models to other common approaches to dealing with missing data on simulated data. I then discuss recommendations for when one should use the reigning popular techniques or the random forest application in the `MICE` procedure.

-->

# Types of missing data: MCAR, MAR, and MNAR

Missing data are not equal. The first form missing data takes is Missing Completely At Random (`MCAR`). This means that the data generating process for the missingness is random. The second form, Missing at Random (`MAR`), refers to data that are missing to some observed cause that can be accounted for. This type of missing data is argued to be the most likely type of data we encounter given factors like the interdependence of outcomes in our social world as well as the size of our datasets that leave fewer unobserved variables on the table [@schunk_2008_atsa]. The third form is called Non-ignorable (`NI`), or sometimes Missing Not at Random (`MNAR`). `MNAR` refers to a pattern of missingness that includes both observable and unobservable causes. @tbl-summary-missing-processes in the Supplementary Materials includes a summary of these, along with their consequences.

# Dealing with missingness

One estimate suggests that 94% of political scientists employ to handle missing data of all types is referred to as Listwise Deletion (`LWD`) [@king_et-al_2001_apsr]. `LWD` is a common default for statistical software that we use for fitting statistical models and essentially excludes observations from our analysis if the dependent variable or any of the covariates contain missing values. In cases of `MCAR` this approach is appropriate as the exclusion of an observation from an analysis would be random.

If the data are `MAR` or `MNAR`, deleting observations with missing data introduces bias in one's regression estimates through a failure to account for correlation between the independent variable(s) and the error [@king_et-al_2001_apsr; @azur_et-al_2011_ijmpr]. Furthermore, it has the potential to decrease statistical power. A meta-analysis of comparative and international political economy papers that use `LWD` demonstrates that political scientists have much, upwards of 50%, more Type I error - an incorrect rejection of the null hypothesis - than we would expect as a result of how we implement `LWD` [@lall_2016_pa]. @fig-bias visualizes this with a DAG.

```{mermaid}
%%| label: fig-bias
%%| fig-cap: MAR data as confound
%%| fig-width: 3
%%{init: {'theme':'base', 'themeVariables':{'primaryColor':'#ffffff', 'primaryBorderColor':'#000000'}}}%%

graph LR;
    a[X<sub>m</sub>] --> b[Y]
    c[X<sub>c</sub>] --> a
    d[M] --> a
```

An improvement calculates the mean or median values of the column in cross-sectional settings or, in panel settings, to take the mean of that variable over time. These interpolation and extrapolation approaches are an improvement over `LWD` in that it uses information from complete cases to provide an estimate of what that missing value should be. In cross-sectional and panel data, these approaches still have some limitations as it makes strong assumptions about the comparability of that cell with the others.

With Hot Deck imputation, you replace a missing value for a column with that of an observation that has similar observed characteristics [@andridge_review_2010]. As you are deciding the imputed values based on observed data, theoretically it is desirable for situations where you have data that are `MAR`. Like matching methods, there is significant debate about the best way to determine whether another observation is similar enough to the one with the missing data [@andridge_review_2010]. Also in datasets where missing data are common, it becomes tricky to find complete observations that are similar to those that are incomplete.

`MI` seeks to solve these issues by imagining the task of imputation in a prediction framework. This approach often uses some implementation of Multi-Chain Monte Carlo (`MCMC`) to leverage the other variables in the dataset to generate a joint posterior distribution of all possible missing values for that particular observation. A popular implementation of `MI` in political science is the `AMELIA II` software [@honaker_et-al_2011_jss]. This useful tool provides a computationally fast and simple process for imputation by taking advantage of bootstrapping with the EMB algorithm. Compared to the other approaches to missing data, `AMELIA II` performs quite well [@honaker_et-al_2011_jss]. `MI`, however, often requires a set of distributional assumptions for the joint distribution - often the multivariate normal [@honaker_et-al_2011_jss]. Another challenge with this tool is that it runs up with the curse of dimensionality -- if you are asking for more information by using more variables than you have observations, many non-regularized models will provide inaccurate estimates.

Multiple Imputation through Chained Equations (`MICE`) is a variant of `MI` that seeks more computational efficiency and loosens some requirements. `MI` struggles to impute values when there is missingness in the other variables of the dataset as it estimates imputed variables based on the joint distribution as opposed to focusing each imputation by optimizing predictions for each variable [@kropko_et-al_2014_pa]. `MICE` tries to get around this limitation in a few steps, as described by @azur_et-al_2011_ijmpr. @fig-process provides a visual representation of the procedure for a form of `MICE` used in this manuscript. I include more details about `RF` in the following subsection.

```{mermaid}
%%| label: fig-process
%%| fig-cap: Steps of RF-MICE procedure
%%| fig-width: 5
%%{init: {'theme':'base', 'themeVariables':{'primaryColor':'#ffffff', 'primaryBorderColor':'#000000'}}}%%
graph TD;
A[Missing Data] --> B[Simple interpolation for each variable]
B --> C[Select Variable and remove imputed values]
C --> D[Random forest model to impute predicted values]
D --> |Repeat for each variable, recommended to be about 5-10 times| C
```

The advantage of this chained equation procedure is to estimate each variable as an outcome with its own regression model that is most appropriate for it. This means that the imputation task optimizes on each variable containing `MAR` data as opposed to optimizing the task for the whole dataset. As `MICE` is regression-based, options for the underlying algorithm to estimate those imputed values are as numerous as our choices for regular statistical analysis. The focus of this manuscript is to examine an extremely flexible regression technique called `RF`.
<!-- 
    END OF LIT REVIEW
-->

# The utility of random forest models for imputing political science data

`RF` are a special type of ensemble supervised machine learning models. For those who would like an in-depth introduction of random forests and supervised machine learning, I recommend @james_et-al_2013_springer. Here, I will provide a simple analogy that outlines the intuition behind what supervised machine learning is to then follow-up with a discussion about how we can apply this intuition to solve missing data problems.

If we are looking to sort beans into a good or bad pile before we toss them into a pot, we often want to collect information about them. Features like color, size, and plumpness can all be good indicators of whether a bean will taste good or bad. Say we have over 5,000 beans, and we are a chef at a restaurant approaching the dinner hour, and we do not have time to sort all these beans. To save time, we look at these features like color, size, and plumpness and make two piles - good or bad for a subset of our 5,000 beans; say, in this instance, about 25%. We then get one of our employees to sort the rest of the beans for us. This employee may know less than us about what features matter more and how to identify a bean that will taste good or bad. Nevertheless, they can look at the two piles we have already made and try to pick up on patterns that make good and bad beans different from one another. With this information, the employee can "learn" these patterns so that even without the same knowledge as the chef, they can still make predictions about whether a bean will taste good or bad.

While this is a very simplistic analogy of how the broader class of supervised machine learning models work, this describes the same task that `RF`, a special type of supervised machine learning, are optimized to perform. These `RF` are optimized through ensemble-based methods to do complete this task recursively and to optimize each iteration by learning from the other iterations. Once this model is trained to determine these patterns, if the model was trained well, it will provide an optimal prediction of what a fixed, but unobserved, observation should be based on the data provided after training.

In a missing data scenario, we can think of the task as training the random forest model on our complete observations in the dataset. In the `MICE` framework, we will fit a random forest model to predict the the values that are originally coded as missing for each variable. We use observations with complete data to train these models so that the weights placed on each predictor variable lead to a model that provides predicted values close to the observed values. Once we have trained this random forest model, we will need to use interpolation on the predictor variables to give us a reasonable data point so that we can use the full information of our data to come up with a predicted value for the column that we are imputing. We do these steps for each variable in our dataset. However, once we have done a full loop through our dataset, we now have predicted values that are better than those interpolated values that we first had to plug-in for the predictor variables for a given imputation model. With `MICE` we do multiple loops through the dataset to reduce the bias that the interpolated starting values may have generated in our imputed value. Intuitively, the more times we impute the whole dataset, the more accurate our imputed values will become. However, with many simulation studies on different types of data and under different circumstances, the common recommendation is that you only need to produce between 5-10 versions of your imputed dataset before the bias generated from those interpolated initial values become relatively inconsequential [see @azur_et-al_2011_ijmpr for a discussion]. Though the exact number of recommended imputed datasets is up for debate and is heavily dependent on a user's computational resources.

This is not the only option at our disposal, however. We can use a Bayesian implementation of a linear regression in the `MICE` framework as well. The processes to impute the data are mostly the same. However, linear regression and `RF` are designed for different tasks. Linear regression models are optimized to help us determine whether a predictor explains an outcome once we account for other variables. `RF`, however, are optimized to learn patterns from existing data, then to use new information to predict some outcome. Furthermore, linear regression has a number of practical limitations. Linear regression is designed to provide predicted values that can range from $-\infty$ to $\infty$. `RF`, on the other hand are extremely flexible in that they can produce predicted values for continuous, categorical, and binary outcomes. An additional practical advantage that I'll mention here is that `RF` are non-parametric models which provide a distinct degree of flexibility to estimate the underlying DGP of the missingness over Generalized Linear Models. With all these considerations together, we may expect that random forests implemented in the `MICE` framework (`RF-MICE`) are extremely flexible and useful tools to solve missing data problems.

<!--
Random forest models are concerned with calculating a fixed out-of-sample prediction under the supervised machine learning framework. They are popular among data scientists and researchers primarily interested in providing predictions instead of engaging in causal inference. In non-imputation applications of supervised machine learning models - a broader category of machine learning models that includes random forests, these models take partial, observed information about an outcome and estimate the relationship between the units with observed outcomes and several other observed features for those units. Then for the units without an observed outcome, we generalize that relationship to predict an outcome for them.

To get an intuition of the fundamental goal of a supervised machine learning model, I will provide an analogy. If we are looking to sort beans into a good or bad pile before we toss them into a pot, we often want to collect information about them. Features like color, size, and plumpness can all be good indicators of whether a bean will taste good or bad. Say we have over 5,000 beans, and we are a chef at a restaurant approaching the dinner hour, and we do not have time to sort all these beans. To save time, we look at these features like color, size, and plumpness and make two piles - good or bad for a subset of our 5,000 beans; say, in this instance, about 25%. We then get one of our employees to sort the rest of the beans for us. This employee may know less than us about what features matter more and how to identify a bean that will taste good or bad. Nevertheless, they can look at the two piles we have already made and try to pick up on patterns that make good and bad beans different from one another. With this information, the employee can "learn" these patterns so that even without the same knowledge as the chef, they can still make predictions about whether a bean will taste good or bad.

We have some units with recorded observations for a particular variable for imputation. Leveraging this, we can treat these documented observations as information so that we can "train" our computer to find a relationship between the observed information in that variable and information from other variables for that unit. We can then generalize this relationship to predict what that value would be for a missing unit. This seems like a reasonable approach to thinking about imputing missing values. We are not making naive imputations by taking the mean value. As we are using an expanded form of `MI`, we can also use all variables in the dataset to clarify that pattern. As a `MICE` procedure, we can specify a machine learning model, and we do this iteratively so that if we have more than one variable that we want to impute, we are not limited to accurate imputed values only for the units that are complete except for that particular variable to be imputed. I will elaborate more on how this works in a few paragraphs. Before we go there, however, I want to take the time to provide a few more details about how random forest models work, as they do not represent the whole of supervised machine learning.

Practitioners refer to random forests as tree-based ensemble models. As a supervised machine learning model, they start with the basic intuition described above; but in the process of "learning" or "training," these models follow a few distinct steps. Decision tree models split regions of the predictive space. For example, say we want to predict vote choice by one's partisanship. We would split the predictive space into regions. These regions could be different degrees of partisanship, like strong Republicans and weak Republicans. When we split this predictive space into regions, we essentially are subsetting our dataset of partisans into these different areas of the predictive space and trying to optimize a model to provide the best predictions in each region in our predictive space. That is, we try to find a model that maximizes the predictive performance of vote choice for strong Republicans, weak Republicans, independents, and so on. We examine performance by comparing how well we are predicting the unobserved outcomes relative to the observed outcomes within those predictive regions. We can "bag" our trees. When we "bag" the decision trees, we bootstrap the training sample and build a tree for each bootstrapped sample and average across them. Doing this allows for a decrease in the variance of the predictions coming from the model, which helps with reducing the chances of generating a trained model that will fail to adequately generalize to our data set not included in the training step.

As this is a primer for the applied researcher, I note that this discussion simplifies decision trees and random forest models. For those interested in more details about these concepts, @james_et-al_2013_springer provide a helpful discussion of these concepts. The main point is that random forest models specialize in generating predictions by optimizing at the *value* and *not* the *variable* level. This characteristic suggests that these models have the potential to be powerful tools for many different applications.

We can discuss their applicability to imputation in the `MICE` framework with a foundation in how random forest models work. As random forest models specialize in generating fixed out-of-sample predictions, these models have a joint goal with multiple imputation in that it should not be evaluated on the model's correctness in terms of explanation of the `MAR` process, but on the model's ability to predict a fixed (true) value [@rubin_1996_jasa]. Here, one might think of missing data as the out-of-sample predictions intended to be estimated. With random forest models, you train the model on a training data set (often randomly generated through cross-validation), a randomly selected portion of your data that you train the model on, and then fit the model on the testing set, the remaining data not used for the training stage of your model [@hastie_et-al_2009_springer].

OLS models often perform relatively poorly on making out-of-sample predictions as they are `BLUE`, assuming the data on hand are relatively representative of the population. As we have `MAR` data, this assumption likely fails, and any out-of-sample predictions are likely to be biased due to overfitting. Further, OLS may also generate out-of-bounds predictions for non-continuous data [@long_1997_sage; @gelman_et-al_2021_cup] which is particularly troublesome in settings of prediction.

Other models provide within-bounds predictions, such as logistic models; however, as generalized linear models, they still assume a linear functional form and often produce biased interpretations of the likelihood function when presented with unobserved systematic processes [@mood_2010_esr]. Though OLS and Logistic regression underlie a lot of machine learning as tools, many consider them to have limited applicability to complicated settings requiring prediction. Random forest models provide within-bounds predictions, and they are fully non-parametric [@hastie_et-al_2009_springer], meaning they do not assume a functional form and consequently a joint distribution. This means that we have more flexibility in terms of what variables we have in our datasets that we need to impute. As social scientists, we rarely have datasets that contain `DGP`s that fall neatly within the optimal realm for `GLM`s. This added flexibility by using `MICE` and random forests makes the researcher's job easier.

On other performance metrics, random forest models, as an ensemble method, provide much more accurate predictions than single-tree alternatives in machine learning, such as CART [@montgomery_olivella_2018_ajps]. As discussed above, rather than generating a single estimate from a single model, ensemble models, like random forests, calculate multiple models and learn from their performance; this is the purpose of using the bootstrapped samples.

In the context of using `MICE`, I argue that political scientists should *consider* using random forest models to make accurate predictions with fewer assumptions and be more lenient in terms of conditioning on the cause of `MAR` in the data set. Recall that within each `MICE` iteration, one performs a model predicting (imputing) a missing value based on the other variables in the dataset. Explicitly, this means you are running a model per variable with missing data.

Given that random forests are non-parametric, within each `MICE` iteration, the relationship between the variable to be imputed and those used to make the predictions can be non-linear and can take many different multivariate distributional forms. This is a significant advancement on traditional `MI`, which assumes a multivariate normal distribution. Additionally, this is an advancement on other `MICE` models that political scientists use, which may not inherently conceptualize missing data as these unobserved values to predict from a generalized relationship of the observed variable with the other variables in the dataset. These relationships are also not assumed to be linear. Furthermore, using `RF-MICE` has the advantage over hot-deck procedures for those unsure about a variable's precise DGP for MAR.

In the next section, I illustrate the use of random forest models for political scientists by demonstrating a simulated application of a random forest implementation of `MICE`. The following section also compares this implementation's performance to other common approaches to handling missing data in political science in terms of our ability to reduce unobserved bias in our data and in computational costs. 
<!--
    END OF KEY SECTION FOR ARGUMENT
-->

# The performance of `RF-MICE` with simulations

``` {r}
#| label: simulation-r-setup-block
#setwd("./src/")
# Set seed  
set.seed(90210)
# Modularly load required functions
box::use(
  tictoc = tictoc[...]
  , ggplot2 = ggplot2[
    ggplot
    , geom_density
    , aes
    , theme_minimal
    , labs
    , geom_boxplot
    , geom_text
  ]
  , infer[
    rep_sample_n
  ]
  , miceRanger[
    amputeData
  ]
  , ./R/simulate[
    simulate
  ]
  , ./R/sim_ridgeline[
    sim_ridgeline
  ]
  , ./R/impute[
    impute
  ]
  , ./R/discrepancy[discrepancy]
)
require(data.table)# for miceRanger

# Loop parameters
N <- 1000000 # size of population data
n <- 100 # size of each sample
datasets <- 1000 # size of number of samples
```


I simulate a population where $N = 1000000$. The population has 5 variables (excluding a row index variable). The data generating process (DGP) of these variables are presented in @eq-sim-dgp. I then take 1000 random samples from that population where the size of each sample is $n = 100$. 

``` {r}
#| label: simulate samples

if (params$reproduce == TRUE) {
  # Generate population data
  df_pop <- simulate(N = N)
  # Generate 1000 random samples
  df_samples <- infer::rep_sample_n(
    tbl = df_pop
    , size = n
    , replace = TRUE
    , reps = datasets
  ) |>
  as.data.table()
  df_samples <- df_samples[
    , dataset := factor(replicate)
  ][
    ,replicate:=NULL
  ]
  # store the population and sample data.frames
  save(
    df_pop
    , df_samples
    , file = "../data/sim_pop_and_samples.RData"
  )
} else {
  load(
    file = "../data/sim_pop_and_samples.RData"
  )
}
```

$$
\begin{split}
a_i = Gamma(2,2) \\
b_i = Binomial(1,0.6) \\
x_i = 0.2 \times a_i + 0.5 \times b_i + Normal(0,1) \\
z_i = 0.9 \times a_i \times b_i + Normal(0,1) \\
y_i = 0.6 \times x_i + 0.9 \times z_i + Normal(0,1)
\end{split}
$$ {#eq-sim-dgp}


For each sample, I use the `miceRanger` [@miceRanger] package to "ampute" the data to introduce missingness for 40% of the observations that follows a `MAR` pattern. The documentation of this package suggests that the amputation utilizes a logistic regression to generate a `MAR` pattern for each variable. This is advantageous to the `mice` [@mice] package's amputation function, which forms the `MAR` pattern for each variable with a linear regression, as it provides a more complex `MAR` pattern for the tools to solve. As tools such as AMELIA assume a MVN for the imputation, I would not expect that there would be many differences in performance between different imputation procedures. Complicating the `MAR` process should help distinguish the limitations and benefits of the imputation procedures; though, it should not cause dramatically more complex processes. For those interested, distributions of the original sample data and the amputed sample data are included in the Supplementary Materials.

With these amputed datasets, I then apply some of the procedures I have discussed to impute these values. Interpolation is a quite simple procedure where I can fill in missing values by using the mean value of that particular variable for the non-missing observations. I perform this interpolation with the `mice` package [@mice] and iterate over it to provide 10 datasets. I also use the `AMELIA II` package [@honaker_et-al_2011_jss] to perform standard `MI` and also store 10 datasets from the iterations. I use a standard Bayesian linear model in the `MICE` framework with the `mice` package [@mice]. As discussed before, the final procedure I use is a `RF` in the `MICE` framework. I perform the `RF-MICE` procedure using the `mice` package [@mice] and an alternative package `miceRanger` [@miceRanger]. I provide an example code block to do this in the supplementary information.

When producing the imputed datasets, I use the `tictoc` [@tictoc] package to record the amount of time each procedure takes to complete the task on the 1000 samples as a measure of computational cost.[^5] As a number of factors may affect the absolute computational costs for these procedures (e.g., hardware, whether other applications or software are running, whether one uses parallelization, etcetera), I am primarily going to focus on the relative computational costs of each procedure.

[^5]: It is important to note that these benchmarks are based on a computer 16 GB of RAM, with a Apple Silicon M2 Pro Processor and a 10-core graphics card.

```{r}
#| label: r-imputations

# Define list of procedures
list_procedure <- c(
  "mean"
  , "Amelia"
  , "norm"
  , "rf"
  , "miceRanger"
)
if (params$reproduce == TRUE) {
  # Interpolation
  tic.clearlog() # clear the log for tic
  tic("Interpolate") # initialize time counter for Interpolate procedure
  list_interpolate <- impute(
    data_frame = df_amputed
    , package = "mice"
    , meth = "mean"
  )# perform interpolation imputation with mice package
  toc(
    log = TRUE
    , quiet = TRUE
  )# document how much time it took for this to complete
  
  # AMELIA
  tic("Amelia") # initialize time counter for amelia procedure
  list_amelia <- impute(
    data_frame = df_amputed
    , package = "Amelia"
  )# perform amelia imputation
  toc(
    log = TRUE
    , quiet = TRUE
  )# document how much time it took for this to complete

  # LMICE
  tic("LMICE") # initialize time counter for linear mice procedure
  list_lmice <- impute(
    data_frame = df_amputed
    , package = "mice"
    , meth = "norm"
  )#perform imputation with linear mice
  toc( 
    log = TRUE
    , quiet = TRUE
  )# document how much time it took for this to complete

  # RFMICE
  tic("RFMICE") # initialize time counter for rfmice procedure
  list_rfmice <- impute(
    data_frame = df_amputed
    , package = "mice"
    , meth = "rf"
  )# perform rfmice imputation with mice package
  toc(
    log = TRUE
    , quiet = TRUE
  )# document how much time it took for this to complete

  #RFRanger
  tic("RFRanger") # initialize time counter for rfmice procedure
  list_rfranger <- impute(
    data_frame = df_amputed
    , package = "miceRanger"
  )# perform rfmice imputation with miceRanger package
  toc(
    log = TRUE
    , quiet = TRUE
  )# document how much time it took for this to complete

  # Store these into one list
  list_imputed <- list(
    mean=list_interpolate
    , Amelia=list_amelia
    , norm=list_lmice
    , rf=list_rfmice
    , miceRanger=list_rfranger
  )
  # Convert each nested list into data.tables
  list_imputed_df <- lapply(
    list_imputed
    , function(x) {
      df_temp <- data.table::rbindlist(
        x
        , id = "dataset"
      )
    }
  )
  # Store it into a data.table
  df_imputed <- rbindlist(
    list_imputed_df
    , id = "procedure"
  )
  df_imputed <- df_imputed[
    , dataset := as.integer(dataset)
  ]
  #Organize timing info into data.frame
  df_log <- tictoc::tic.log(
    format = TRUE
  ) |> # take the logged timing info and store it as a data.frame object
    stringr::str_split(
        ": "
        , simplify = TRUE
    ) |>
    data.table()
  df_log <- df_log[
    , V2 := gsub(
      "[sec elapsed]"
      , ""
      , as.character(V2)
      )
  ][
    , V2 := as.double(V2)
  ]

  setnames(  # set the column names
    df_log, # of the log dataset
    c("procedure","seconds") # to procedure and seconds
  )

  # Store info
  save(
    df_imputed,
    df_log,
    file = "../data/sim_imputed_and_log.RData"
  )
} else {
  load(
    file = "../data/sim_imputed_and_log.RData"
  )
}
```

Each imputation procedure produced $m = 10$ datasets per simulated dataset, $s = 1000$. I have a total of $s \times m$ datasets. For each s dataset, I took the difference the values for each m dataset from the values in the complete dataset and took the average of these differences across the 10 imputed, m, datasets for each sample to give me a mean score of the discrepancy for each s dataset. @fig-discrepancy represents these mean discrepancy scores for the three variables that were originally imputed. The text on each boxplot represents the median sample's average discrepancy for that particular procedure.

```{r}
#| label: calculating-average-discrepancies

# calculate the discrepancy
list_discrepancy <- lapply(
  list_procedure
  ,function (x) {
    discrepancy(
      sample_data = df_samples
      , imputed_data = df_imputed
      , impute_type = x
      , model = FALSE
      , ct_type = "mean"
        )
    }
)

# Define names of the list elements
base::names(list_discrepancy) <- list_procedure

# Convert list_discrepancy into data.frame
df_discrepancy <- data.table::rbindlist(
    list_discrepancy
    , idcol = "procedure"
)
# Convert procedure column of discrepancy data.frame to factor
df_discrepancy <- df_discrepancy[
  , procedure := fcase(
    procedure == "Amelia", "Amelia",
    procedure == "mean", "Mean",
    procedure == "miceRanger", "miceRanger (RF-MICE)",
    procedure == "norm", "L-MICE",
    procedure == "rf", "RF-MICE"
  )
][
  , procedure := factor(procedure)
]
```

```{r}
#| label: fig-discrepancy
#| layout-nrow: 3
#| fig-height: 2.75
#| fig-cap: Discrepancies between original and imputed data
#| fig-subcap:
#|   - X variable
#|   - Z variable
#|   - Y variable
# Plot discrepancies for X variable
  #* calculate median
median_x <- aggregate(X ~ procedure, df_discrepancy, median)
plot_discrepancy_x <- ggplot() +
  geom_boxplot(
    aes(
      x = procedure
      , y = X
    )
    , data = df_discrepancy
  ) + 
  geom_text(
    data = median_x
    , aes(
      label = round(X, digits = 2)
      , x = procedure
      , y = X + 1
    )
    , size = 6
  ) + 
  theme_minimal() + # add the minimal theme
  labs(
    x = 'X',
    y = 'Average difference'
  ) # adjust the labels to the axes

# Plot discrepancies for Z variable
  #* calculate median
median_z <- aggregate(Z ~ procedure, df_discrepancy, median)
plot_discrepancy_z <- ggplot() +
  geom_boxplot(
    aes(
      x = procedure
      , y = Z
    )
    , data = df_discrepancy
  ) + 
  geom_text(
    data = median_z
    , aes(
      label = round(Z, digits = 2)
      , x = procedure
      , y = Z + 1
    )
    , size = 6
  ) +
  theme_minimal() + # add the minimal theme
  labs(
    x = 'Z',
    y = 'Average difference'
  ) # adjust the labels to the axes

# Plot discrepancies for Y variable
median_y <- aggregate(Y ~ procedure, df_discrepancy, median)
plot_discrepancy_y <- ggplot() +
  geom_boxplot(
    aes(
      x = procedure
      , y = Y
    )
    , data = df_discrepancy
  ) + geom_text(
    data = median_y
    , aes(
      label = round(Y, digits = 2)
      , x = procedure
      , y = Y + 1
    )
    , size = 6
  ) +
  theme_minimal() + # add the minimal theme
  labs(
    x = 'Y',
    y = 'Average difference'
  ) # adjust the labels to the axes

plot_discrepancy_x
plot_discrepancy_z
plot_discrepancy_y
```

Overall, the procedures do quite well in that the average difference between the actual data and the imputed data are quite small across the datasets. We see that `RF-MICE` when implemented with `miceRanger` [@miceRanger] consistently does a good job at coming closer to the correct value than the other procedures do. `RF-MICE` when implemented in the `mice` [@mice] package does a poorer job at this, however. In terms of speed to execute the imputation, interpolation with the `mice` package took an average of `r sprintf('%.3f', df_log[1,2]/1000)` seconds; Amelia took an average of `r sprintf('%.3f', df_log[2,2]/1000)` seconds; `Linear-MICE` took an average of `r sprintf('%.3f', df_log[3,2]/1000)` seconds; `RF-MICE`, as implemented by `mice` [@mice], took an average of `r sprintf('%.3f', df_log[4,2]/1000)` seconds; and `RF-MICE`, as implemented by `miceRanger` [@miceRanger], took an average of `r sprintf('%.3f', df_log[5,2]/1000)` seconds.

Though it is not a novel claim, I argue that in situations where we have missingness due to a `MAR` pattern, our regression models suffer from bias due to the systematic process generating that missingness. What I have argued so far is that we should consider using `RF-MICE` as it is, relative to other `MI` tools, a flexible tool that may be able to model a number of `MAR` processes which would help with reducing bias in our regression models.

To examine this claim, I take the amputed and imputed datasets (10) and use Rubin's rule [@rubin_1996_jasa] to pool across the regression models performed on each amputed and imputed sample. I then calculate the discrepancy by subtracting the parameter value from the point estimate. 

```{r}
#| label: calculating-average-model-discrepancies

# calculate the discrepancy
list_model_discrepancy <- lapply(
  list_procedure
  ,function (x) {
    discrepancy(
      sample_data = df_samples
      , imputed_data = df_imputed
      , impute_type = x
      , model = TRUE
      , ct_type = "mean"
        )
    }
)

# Define names of the list elements
base::names(list_model_discrepancy) <- list_procedure

# Convert list_discrepancy into data.frame
df_model_discrepancy <- data.table::rbindlist(
    list_model_discrepancy
    , idcol = "procedure"
)
# Convert procedure column of discrepancy data.frame to factor
df_model_discrepancy <- df_model_discrepancy[
  , procedure := fcase(
    procedure == "Amelia", "Amelia",
    procedure == "mean", "Mean",
    procedure == "miceRanger", "miceRanger (RF-MICE)",
    procedure == "norm", "L-MICE",
    procedure == "rf", "RF-MICE"
  )
][
    , procedure := factor(procedure)
]
```

@fig-model-discrepancy present the distribution of differences between my point estimate and of my parameter value for the $\beta$ coefficient for `X` and for `Z` respectively. This figure demonstrates, again, that `RF-MICE`, as implemented in `miceRanger` [@miceRanger], does a relatively good job at reducing levels of bias in my eventual statistical analyses. `RF-MICE` through the `mice` [@mice] package, however, performs worse than AMELIA and `Linear-MICE`.

```{r}
#| label: fig-model-discrepancy
#| layout-nrow: 2
#| fig-height: 2.75
#| fig-cap: Discrepancies of estimates between original and imputed data
#| fig-subcap:
#|   - X variable
#|   - Z variable
#|   - Y variable
# Plot discrepancies for X variable
median_model_x <- aggregate(X ~ procedure, df_model_discrepancy, median)
plot_model_discrepancy_x <- ggplot() +
  geom_boxplot(
    aes(
      x = procedure
      , y = X
    )
    , data = df_model_discrepancy
  ) + 
  geom_text(
    data = median_model_x
    , aes(
      label = round(X, digits = 2)
      , x = procedure
      , y = X + 1
    )
    , size = 6
  ) +
  theme_minimal() + # add the minimal theme
  labs(
    x = 'X',
    y = 'Average difference'
  ) # adjust the labels to the axes

# Plot discrepancies for Z variable
median_model_z <- aggregate(Z ~ procedure, df_model_discrepancy, median)
plot_model_discrepancy_z <- ggplot() +
  geom_boxplot(
    aes(
      x = procedure
      , y = Z
    )
    , data = df_model_discrepancy
  ) + 
  geom_text(
    data = median_model_z
    , aes(
      label = round(Z, digits = 2)
      , x = procedure
      , y = Z + 1
    )
    , size = 6
  ) +
  theme_minimal() + # add the minimal theme
  labs(
    x = 'Z',
    y = 'Average difference'
  ) # adjust the labels to the axes

plot_model_discrepancy_x
plot_model_discrepancy_z
```

# Conclusions

In theory, `RF-MICE` is a quite flexible tool that can operate in a number of circumstances to not only discover systematic processes leading to missingness in one's data, but to also use such information to recover the values. These expectations rely on the claim that `RF` are optimized for discovering patterns and to use that information to make out-of-sample predictions. With `RF-MICE` Random forests are coupled with `MICE` to produce significant improvements at retrieving the true values when a `MAR` process is present.

Using simulated data, I demonstrate two implementations of `RF-MICE` in `R` and compare it to implementations of more common procedures for dealing with `MAR` processes in political science. Overall, my simulated data and a number of measures of performance favor those theoretical expectations.

When comparing the distribution of imputed samples to the true samples, as if the `MAR` process was not present, `RF-MICE` implemented with `miceRanger` does quite well in recovering the true values that were missing. When examining the discrepancy between what the imputed value is and the true value, the two `RF-MICE` implementations provide quite small discrepancies on average.

Measuring the performance of `RF-MICE` in terms of reducing bias in statistical estimation, I find that `RF-MICE`, when implemented with the `miceRanger` package [@miceRanger], stands out as a valuable tool for researchers to use to reduce bias generated with `MAR` processes.

While in theory, it is nice for the applied researcher to hear about a new and powerful tool or procedure, they face many constraints. When examining how much time each implementation of a procedure took, I observed that both implementations of `RF-MICE` were not significantly worse in time it took to complete the procedure relative to other multiple imputation procedures.

Altogether, the simulated data suggest that `RF-MICE` is a valuable procedure for the applied researcher's toolbox. When dealing with missing data that one suspects may not be the result of a `MCAR` process, one should consider the flexibility that `RF-MICE` provides. It does not require the researcher to specify the variables involved in the `MAR` process, but also does not introduce significant computational costs nor introduce so much complexity as a procedure that it is impossible to anticipate or diagnose problems the procedure may introduce.

It is important to remind the reader that all tools have their limitations and that their value are quite dependent on the context for which they are to be applied. It is useful, however, to include tools that vary in the assumptions they make [@neumayer_plumper_2017_cup]. The capabilities of `RF-MICE` are no different. While `RF-MICE` is quite flexible for addressing a number of `MAR` processes, it is important to note that it is not and should not be seen as a default or the sole tool for one to use when dealing with missing data. For example, simulations demonstrate that `RF-MICE` performs poorly when the missing data pattern arises from a moderating relationship [@marbach_2022_pa]. With the number of tools and their implementations being so available to the applied researcher, one should not shy away from using multiple implementations of these tools to ensure that one's substantive conclusions are not dependent on one tool or implementation. 
<!--
    END OF DOCUMENT
-->

# References
::: {#refs}
:::

# Supplementary Materials {.appendix}

## Summaries of types of missing data and solutions {.appendix}

| Type | Cause                                                                                                                                               | Problem                                                                                     |
|-----------------|------------------------------|-------------------------------|
| MCAR | Missing data patterns are stochastic                                                                                                                | Does not cause bias in estimates                                                            |
| MAR  | Missing data patterns are not stochastic; however, once accounting for observed causes of missingness, any remaining missing data are as-if random. | Generates a type of omitted variable bias without accounting for it.                        |
| MNAR | Missing data patterns are not stochastic; caused by unobserved sources.                                                                             | Generates a type of omitted variable bias; hard to correct for as the source is unobserved. |

: Types of missing data processes, problems, and solutions {#tbl-summary-missing-processes}

| Procedure | Assumptions                                                                                                                                                                                                                                                                                                                                 | What it does                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Fixes                                       |
|--------------------|----------------------|-----------------------------------|--------|
| LWD       | Sources of missingness are random.                                                                                                                                                                                                                                                                                                          | Removes rows that have a missing value for *any* variable in the statistical model                                                                                                                                                                                                                                                                                                                                                                                                                                                             | MCAR                                        |
| Simple    | Sources of missingness may be explained by some type of autocorrelation (spatial, temporal, non-independence of observations).                                                                                                                                                                                                              | Takes the average or median value of observations that occur either before (if temporal data), proximate to (if spatial data), or similar to (if in the same sample), and fills that value into all rows with missingness for that variable.                                                                                                                                                                                                                                                                                                   | Data with autocorrelation or are not i.i.d. |
| Hotdeck   | Regression-based. So assumptions made are dependent on the particular regression you choose for this procedure.                                                                                                                                                                                                                             | Fits a regression model (specified by the researcher) where the researcher regresses the variable with missing values on other columns in the dataset. Fills empty rows with their predicted values from the regression model. Assumes that the regression model is properly specified.                                                                                                                                                                                                                                                        | MAR                                         |
| MI        | Assumes a normally distributed joint posterior distribution. Assumes that a GLM is appropriate for describing the non-stochastic sources of missingness.                                                                                                                                                                                    | Fits a Bayesian Linear Regression. Iteratively regresses each variable containing missing values onto all other variables in the dataset. Completes this process multiple times to account for uncertainty in the particular construction of the posterior distribution.                                                                                                                                                                                                                                                                       | MAR                                         |
| RFMICE    | MICE, in general, is constrained by the assumptions of the particular estimator one chooses. RFMICE, however, is designed to loosen a number of assumptions. Still in the Frequentist framework so it does not produce posterior distributions to reflect uncertainty. Uncertainty is reflected by variation within and between iterations. | First performs simple imputation on each variable that has missingness. Then fits a random forest model where it attempts to predict each variable containing missingness using all other variables present in the dataset. Within each iteration, `RF` perform their own iterative procedures to maximize predictive capacity using cross-validation. It performs this for each variable and produces some user specified number of datasets which are the result of those maximally predicted values for each missing value. | MAR                                         |

: Common solutions for missing data {#tbl-summary-missing-solutions}

## Distributions of simulated data {.appendix}
```{r}
#| label: ampute the data

if (params$reproduce == TRUE) {
  # split these up into a list of data.frames for amputation
  list_ampute_prep <- split(
    df_samples
    , f = df_samples$dataset
  )
  # Perform the amputation on each of the samples
  list_amputed <- lapply(
    list_ampute_prep
      , function (x) {
        df_amputed_temp <- amputeData(
          data = x
          , perc = 0.4
          , cols = c("X","Z","Y")
        )
      }
  )
  # Combine the list of data.table objects into one data.table
  df_amputed <- data.table::rbindlist(list_amputed)
  # Store it in a file
  save(
    df_amputed
    , file = "../data/sim_amputed.RData"
  )
} else {
  load(
    file = "../data/sim_amputed.RData"
  )
}

```

I use the `ggplot2` [@ggplot2] and `ggridges` [@ggridges] packages to display the distributions of the original sample data and the amputed data. As I have 1000 samples, I try to simplify the plots by presenting the density distributions of the X, Y, and Z variables for 10 samples. The density distributions for the original data are presented in @fig-sim-dist and the density distributions for the amputed data are presented in @fig-amputed-dist.

``` {r}
#| label: fig-sim-dist
#| layout-nrow: 1
#| fig-height: 8
#| fig-cap: Distributions of complete sample data
#| fig-subcap:
#|  - X variable
#|  - Z variable
#|  - Y variable
# Generate plots of samples
plot_sample <- lapply(
  c("X", "Z", "Y")
  , function (x) {
    sim_ridgeline(
      data_frame = df_samples
      , str_var = x
    )
  }
)
# Show each plot
plot_sample[[1]]
plot_sample[[2]]
plot_sample[[3]]
```

```{r}
#| label: fig-amputed-dist
#| layout-nrow: 1
#| fig-height: 8
#| fig-cap: Distributions of amputed sample data
#| fig-subcap:
#|  - X variable
#|  - Z variable
#|  - Y variable
plot_amputed <- lapply(
  c("X", "Z", "Y")
  , function (x) {
    sim_ridgeline(
      data_frame = df_amputed
      , str_var=x
    )
  }
)
# Show each plot
plot_amputed[[1]]
plot_amputed[[2]]
plot_amputed[[3]]
```

## Code {.appendix}

```{.r}
# Install libraries
install.packages(
  c(
      "AMELIA"
      , "mice"
      , "miceRanger"
  )
)
# Load libraries
library(Amelia) # for MI
library(mice) # for many MICE and interpolation procedures
library(miceRanger) # for RF-Mice procedure
# Dataset

df

# Listwise Deletion
dfImputed <- df[complete.cases(df), ] # exclude rows that have missing values in any column

# Interpolation
dfImputed <- mice(
  df # dataframe
  , m = 10 # number of imputations
  , method = "mean" # mean interpolation
)

# Amelia
dfImputed <- amelia(
  df # dataframe
  , m = 10 # number of imputations
)

# Linear Bayesian MICE
dfImputed <- mice(
  df # dataframe
  , m = 10 # number of imputations
  , method = "linear" # Bayesian linear MICE
)

# RF-MICE with mice package
dfImputed <- mice(
  df # dataframe
  , m = 10 # number of imputations
  , method = "rf" # RF-MICE
)

# RF-MICE with miceRanger package
dfImputed <- miceRanger(
  df # dataframe
  , m = 10 # number of imputations
)
```